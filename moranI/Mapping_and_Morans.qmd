---
title: "Moran's I analysis in R"
author: "Manny Gimond"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning=FALSE, tidy=FALSE )
```

------------------------------------------------------------------------

[Data for this tutorial can be downloaded [from here](./data.zip). Make sure to unzip the files to a dedicated folder on your computer.]{.block1}

-----

> Don't forget to set the R session to the project folder via *Session \>\> Set Working Directory \>\> Choose Directory*.

-----

We will make use of the following packages: `sf` for importing the shapefiles, `tmap` for creating choropleth maps and `spdep` for implementing the Moran's I analysis. Note that you will probably need to install `spdep` since this package has not been used thus far in class.

The functions in `spdep` use spatial formats that differ from those used with `spatstat`. As such, we will no longer need to rely on calls to `as.owin(s)` and `as.ppp(s)` as was the case with the `spatstat` functions.

```{r}
library(sf)
library(spdep)
library(RColorBrewer)
```


## Loading the data

Don't forget to set the session's working directory to the folder that contains the `NHME.shp` shapefile before running the following chunks of code. 

```{r results='hide'}
s <- st_read("NHME.shp")
```

Note that unlike the earlier point pattern analysis exercises, we will need to _keep_ the attribute information with our spatial objects.

To list the column names associated with the object's attribute table, type:

```{r}
names(s)
```

To list the contents of an attribute, affix the dollar sign `$` to the object name followed by the attribute name. For example, to list the income values, type:

```{r}
s$Income
```

You can plot the attribute values as follows:

```{r, fig.height=2, fig.width=3, echo=2}
OP <- par(mar = c(4,4,1,1))
hist(s$Income, main=NULL)
par(OP)
```

or,
```{r, fig.height=1.3, fig.width=3, echo=2}
OP <- par(mar = c(2,1,0,1))
boxplot(s$Income, horizontal = TRUE)
par(OP)
```

To generate a map by symbolizing the polygons using the `Income` attribute we will define the classification breaks (`breaks = quantile` with `n = 8` breaks) and the symbol colors (`palette="Greens"`). For the latter, the `tmap` package makes use of Cynthia Brewer's color schemes (you are already familiar with her [website](http://colorbrewer2.org/)). 

```{r fig.height=3}
color <- brewer.pal(8, "Greens")
plot(s["Income"], key.pos = 4, nbreaks = 8, breaks = "quantile", pal = color)
```

You can change the classification schemes by setting the `breaks` parameter to styles such as `"equal"`, `"jenks"` (ArcGIS' default), `"sd"`, `"pretty"` to name a few.

You can change the Brewer color palette to any one of the following sequential color schemes:

```{r fig.height=4}
display.brewer.all(type = "seq")
```

## Moran's I analysis

### Step 1: Define neighboring polygons

The first step in a Moran's I analysis requires that we define "neighboring" polygons. This could refer to contiguous polygons, polygons within a certain distance, or it could be non-spatial in nature and defined by social, political or cultural "neighbors".

Here, we'll adopt a contiguous neighbor definition. We'll accept any contiguous polygons that share at least one vertex; this is the "queen" case (if one chooses to adopt the chess analogy) and it's  parameterized as `queen = TRUE` in the call to `poly2nb`. If we required that just *edges* be shared between polygons then we would set `queen = FALSE` (the *rook* case).
 
```{r}
#| class-output: divh3
#| class-source: divh3
nb <- poly2nb(s, queen=TRUE)
```
 
For each polygon in our shape object, `nb` lists all neighboring polygons. For example, to see the neighbors (by ID number) for the first polygon in the shape object, type:
 
```{r}
#| class-output: divh3
#| class-source: divh3
nb[1]
```
 
Here's the list of county names and associated IDs:
 
```{r echo=FALSE}
#| class-output: divh3
library(kableExtra)
knitr::kable(data.frame(County=s$NAME, ID=1:nrow(s)), "html", table.attr = "class=\'divh3\'") %>% scroll_box(width = "200px", height = "200px")  

```

### Step 2: Assign weights to the neighbors

Next, we need to assign weights to each neighboring polygon. In this example, each neighboring polygon will be assigned **equal weight** when computing the neighboring mean income values.

```{r}
#| class-output: divh3
#| class-source: divh3
lw <- nb2listw(nb, style="W", zero.policy=TRUE)
```

To see the weight of the first polygon's neighbors type:
```{r}
#| class-output: divh3
#| class-source: divh3
lw$weights[1]
```

These are the weights each neighboring income value will be multiplied by before being summed. If a polygon has 5 neighbors, each neighbor will have a weight of 1/5 or 0.2. This weight will then be used to compute the mean neighbor values as in `0.2(neighbor1) + 0.2(neighbor2) + 0.2(neighbor3) + 0.2(neighbor4) + 0.2(neighbor5)`. This is equivalent to summing all five income values then dividing by 5.

### Step 3: Compute the (weighted) neighbor mean income values (optional step)

NOTE: This step does not need to be performed when running the `moran` or `moran.test` functions outlined in Steps 4 and 5. This step is only needed if you wish to generate a scatter plot between the income values and their lagged counterpart.

Next, we'll have R compute the average neighbor income value for each polygon. These values are often referred to as **spatially lagged** values.

```{r}
#| class-output: divh3
#| class-source: divh3
inc.lag <- lag.listw(lw, s$Income)
inc.lag
```

You can plot the relationship between income and its spatially lagged counterpart as follows (note that the blue line added to the plot is derived from a regression model).

```{r fig.height=2.5,  fig.width = 2.5, echo=2:3}
#| class-output: divh3
#| class-source: divh3
OP <- par(pty="s", mar=c(3.5, 3.7, 0,1))
plot(inc.lag ~ s$Income, pch=16, asp=1)
abline(lm(inc.lag ~ s$Income), col="blue")
par(OP)
```

### Step 4: Computing the Moran's I statistic

The Moran's I statistic can be computed using the `moran` function. 

```{r}
#| class-output: divh3
#| class-source: divh3
I <- moran(s$Income, lw, length(nb), Szero(lw))[1]
I
```

Recall that the Moran's `I` value is the slope of the line that best fits the relationship between neighboring income values and each polygon's income in the dataset.


### Step 5: Performing a hypothesis test

The hypothesis we are testing states that _"the income values are randomly distributed across counties following a completely random process"_. There are two methods to testing this hypothesis: an analytical method and a Monte Carlo method. We'll explore both approaches in the following example.

#### Analytical method

To run the Moran's I analysis using the analytical method, use the `moran.test` function. 

```{r}
moran.test(s$Income,lw, alternative="greater", zero.policy=TRUE) 
```

The Moran's I statistic is `r round(moran.test(s$Income,lw)$estimate[1],3)` (same value that was computed using the `moran` function as expected). The p-value is very small. Usually, when the p-value is very small it's common practice to report it as `< 0.001`. 

Note that ArcGIS adopts this analytical approach to hypothesis testing however, it implements a **two-sided** test as opposed to the **one-sided** test adopted in the above example (i.e. `alternative = "greater"`). A two-sided p-value is nothing more than twice the one-sided p-value. Unfortunately, ArcGIS does not seem to make this important distinction in any of its documentation. This distinction can have important ramifications as shown in the next example (Florida crime data). The Maine income data is so strongly clustered that both a one-sided and two-sided test produce the same outcome (a p-value close to 0).

#### Monte Carlo method

The analytical approach to the Moran's I analysis benefits from being fast. But it may be sensitive to irregularly distributed polygons. A safer approach to hypothesis testing is to run an MC simulation using the `moran.mc` function. The number of simulations is defined by the `nsim = ` parameter. Here, we'll permute income values `999` times.

The `moran.mc` function takes another parameter called `alternative =`. This parameter has three possible values: `"greater"` (the default), `"less"`, and `"two.sided"`. The choice will be dictated by the side of the distribution we want to compute the p-value for. If our observed Moran's I is to the right of the expected distribution, we will want to adopt the `"greater"` option which will focus on the upper tail of the distribution. If our observed value is to the left of the distribution, we will want to choose the `"less"` option to focus on the lower tail of the distribution. You can usually tell from the computed Moran's I value which tail you will want to emphasize by its sign. A general rule of thumb is to place emphasis on the lower tail if Moran's I value is negative, and to place emphasis on the upper tail if Moran's I value is positive. In our example, out Moran's I value of `r round(unlist(I),2)` is positive so we'll choose `"greater"` for the parameter.

```{r}
MC<- moran.mc(s$Income, lw, nsim = 999, alternative = "greater", zero.policy=TRUE)

# View results (including p-value)
MC
```

The MC simulation generates a very small p-value, `r MC$p.value`. This is not surprising given that the income values are strongly clustered. We can see the results graphically by passing the Moran's I model to the plot function:

```{r fig.height=3}
#| class-output: divh3
#| class-source: divh3
# Plot the Null distribution (note that this is a density plot instead of a histogram)
plot(MC, xlab="Moran's I")
```

The curve shows the distribution of Moran I values we could expect had the incomes been randomly distributed across the counties. Note that our observed statistic, `r round(moran.test(s$Income,lw)$estimate[1],3)`, falls way to the right of the distribution suggesting that the income values are clustered (a positive Moran's I value suggests clustering whereas a negative Moran's I value suggests dispersion).

Can you tell the difference between our observed income distribution and those generated from a completely random process in the following figure?

```{r echo=FALSE, fig.height=2, fig.width=7}
#| class-output: divh3
#| class-source: divh3
set.seed(131)
s$rand1 <- sample(s$Income, length(s$Income), replace = FALSE)
s$rand2 <- sample(s$Income, length(s$Income), replace = FALSE)
s$rand3 <- sample(s$Income, length(s$Income), replace = FALSE)

OP <- par(mfrow=c(1,4))
tmp <- plot(s["Income"], key.pos = NULL, reset = FALSE, main = NULL,
            nbreaks = 8, breaks = "quantile", pal = color)
tmp <- plot(s["rand2"], key.pos = NULL, reset = FALSE, main = NULL, 
            nbreaks = 8, breaks = "quantile", pal = color)
tmp <- plot(s["rand1"], key.pos = NULL, reset = FALSE, main = NULL, 
            nbreaks = 8, breaks = "quantile", pal = color)
tmp <- plot(s["rand3"], key.pos = NULL, reset = FALSE, main = NULL, 
            nbreaks = 8, breaks = "quantile", pal = color)
par(OP)
              
```

The map on the left is our observed distribution. The three maps on the right are realizations of a completely random process.


## Another example: Florida 1980 Homicide rate example

In this example, we explore the spatial distribution of 1980 homicide rates `HR80` by county for the state of Florida using the Monte Carlo approach.  The data are found in the `NAT/` folder used in the in-class exercise.

```{r fig.height=3, echo=FALSE, message=FALSE, results='hide'}
# Load the shapefile
s <- st_read("FL.shp")

# Plot the data
color <- brewer.pal(n=8, "Reds")
plot(s["HR80"], nbreaks = 8, breaks = "quantile", pal = color)
```

The following code chunk highlights the entire workflow (don't forget to set your R session folder to that which houses the `FL.shp` file).

```{r fig.height=3.2, message=FALSE, warning=FALSE,results='hide', fig.show='hold', echo=-1}
set.seed(2354)
# Load the shapefile
s <- st_read("FL.shp")

# Define the neighbors (use queen case)
nb <- poly2nb(s, queen=TRUE)

# Compute the neighboring average homicide rates
lw <- nb2listw(nb, style="W", zero.policy=TRUE)

# Run the MC simulation version of the Moran's I test
M1 <- moran.mc(s$HR80, lw, nsim=9999, alternative = "greater", zero.policy=TRUE)

# Plot the results
plot(M1, xlab = "Moran's I")

# Display the resulting statistics
M1
```

```{r echo=FALSE}
M1
```


The MC simulation generated a p-value of ~0.04 suggesting that there would be a ~4% chance of being wrong in rejecting the null hypothesis or that there is a ~4% chance that our observed pattern is consistent with a random process (note that your simulated p-value may differ from the one shown here--the number of simulations may need to be increased to reach a more stable convergence). Recall that this is a one-sided test. ArcGIS's analytical solution adopts a two-sided test. To compare its p-value to ours, we need to divide its p-value by 2 (i.e. `0.0588 / 2`) which gives us a one-sided p-value of `0.0294`--about 25% smaller than our simulated p-value.

```{r fig.width=5, fig.height=5,echo=FALSE}
library(png)
library(grid)
img <- readPNG("img/HR80_arcmap_output.PNG")
 grid.raster(img)
```

The wording adopted by ArcGIS under the infographic (highlighted in yellow in the above figure) is unfortunate. It seems to suggest a one-sided test by explicitly describing the nature of the pattern (i.e. *clustered*). A more appropriate statement would have been  _"There is less than a 6% likelihood that the observed pattern could be the result of random chance"_ (note the omission of the word _clustered_).
