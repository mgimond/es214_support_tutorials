{
  "hash": "5de4b66365ef75a9cf02670fecd08764",
  "result": {
    "markdown": "---\ntitle: \"ANN Hypothesis Testing: homogeneous case\"\nauthor: \"Manny Gimond\"\n---\n\n\n\n\n------------------------------------------------------------------------\n\n[Data for this tutorial can be downloaded [from here](./walmarts.zip). Don't forget to unzip the files to a dedicated folder on your computer.]{.block1}\n\n-----\n\n> Don't forget to set the R session to the project folder via *Session \\>\\> Set Working Directory \\>\\> Choose Directory*.\n\n-----\n\n## Load and prep the dataset\n\nIn the following chunk of code, we will load the shapefiles into R, then we will convert the spatial objects into formats that are readable by the `spatstat` functions. We will also convert the mapping units from meters to kilometers using the `rescale` function. This last step will generate distance values of kilometers instead of meters when we compute the average nearest neighbor value. Much like ArcGIS, R will adopt the layer's coordinate system's map units when expressing distance or area values, so by changing the layer's default map units, we end up with output distance values that are no more than 3 or 4 digits long and easier to \"interpret\". \n\n\n\n::: {.cell hash='Homogenous_ANN_cache/html/unnamed-chunk-1_1e02d923914b6dd2ed056ec75be696e6'}\n\n```{.r .cell-code}\n# Load packages\nlibrary(sf)\nlibrary(spatstat)\n\n# Read state polygon data\ns  <- st_read(\"MA.shp\")\nw  <- as.owin(s)\nw.km <- rescale(w, 1000)  # rescale map units to km\n\n# Read Walmart point data\ns  <- st_read(\"Walmarts.shp\")\np  <- as.ppp(s)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in as.ppp.sf(s): only first attribute column is used for marks\n```\n:::\n\n```{.r .cell-code}\nmarks(p) <- NULL  # Remove attribute table (simplifies plot operations)\np.km <- rescale(p, 1000) # Rescale map units to km\nWindow(p.km) <- w.km      \n```\n:::\n\n\n## Average nearest neighbor analysis\n\nFirst, we'll compute the observed Walmart ANN statistic.\n\n\n::: {.cell hash='Homogenous_ANN_cache/html/unnamed-chunk-2_ec10735ff3e83ea3c4aff0d9e6806c0e'}\n\n```{.r .cell-code}\nann.p <- mean(nndist(p.km, k=1))\nann.p\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 13.29478\n```\n:::\n:::\n\n\nThe observed average nearest neighbor is 13.29 km.\n\n### Is our observed ANN value consistent with a random process?\n\nIn this hypothesis test, we are hypothesizing that the process that generated the observed distribution of Walmart stores was completely random. This is our **null hypothesis**. We'll therefore compare our observed ANN value to the *range* of ANN values we could expect to get *if* the Walmart stores were randomly distributed. This will involve randomly shuffling the Walmart store locations, then computing the average distance between the randomly distributed stores. This process is then repeated many times such that a distribution of ANN values under the assumption of complete randomness (the null hypothesis) is generated.\n\n\n::: {.cell hash='Homogenous_ANN_cache/html/unnamed-chunk-3_5001c0312a198fc4e8cf3dcd3123d302'}\n\n```{.r .cell-code}\nn     <- 1999       # Number of times to run the simulation\nann.r <- vector()   # Create an empty object to be used to store the simulated  ANN values\n\nfor (i in 1:n){\n  rand.p   <- rpoint(n = p.km$n, win = w.km)  # Generate random point locations\n  ann.r[i] <- mean(nndist(rand.p, k = 1))     # Compute and store the simulated ANN value\n}\n```\n:::\n\n\nIn the above loop, the function `rpoint` is passed two parameters: `n = p.km$n` and `win = w.km`. The first tells the function how many points to randomly place (i.e. the same number of points as that in the Walmart points layer which we can extract from the `p.km$n` object). The second tells the function to confine the randomly generated points to the extent defined by `w.km` (the `MA` polygon).\n\nNote that after running the last simulation, you can view its set of randomly generated points via:\n\n\n::: {.cell hash='Homogenous_ANN_cache/html/unnamed-chunk-4_1e29f3348b16237ee677e4f74bef62d6'}\n\n```{.r .cell-code}\nplot(rand.p, pch = 16, main = NULL)\n```\n\n::: {.cell-output-display}\n![](Homogenous_ANN_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nGiven that this is a random process, your output will look different--as expected.\n\nNext, let's plot the histogram of the simulated ANN values then add a blue line showing where our observed ANN value lies relative to the distribution of simulated ANN values under the null. (Your histogram may look different given the random nature of the simulation).\n\n\n::: {.cell hash='Homogenous_ANN_cache/html/unnamed-chunk-5_c2e9e3814e6e3989aea8c033b493319a'}\n\n```{.r .cell-code}\nhist(ann.r, breaks = 40, col = \"bisque\", xlim = range(ann.p, ann.r), main = NULL)\nabline(v = ann.p, col = \"blue\", lw = 2)  # lw = 2 increases the line width\n```\n\n::: {.cell-output-display}\n![](Homogenous_ANN_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nThe test suggests that our observed ANN value may not be that different from the 1999 ANN values we simulated under the assumption that the stores are randomly distributed. Our observed values is a tad bit to the right of the center of the distribution suggesting that our observed ANN value might be on the dispersed side of the range of values (a larger than expected ANN value suggests a more dispersed set of points, and a smaller than expected ANN value suggests a more clustered set of points).\n\n### Extracting a p-value from the simulation\n\nWe first need to find the end of the distribution that is *closest* to the observed ANN value. We then find the number of simulated ANN values more extreme than our observed ANN value. Finally, we divide that count by the total number of simulations. Note that this is a so-called *one-sided* P-value. See [lecture notes](https://mgimond.github.io/Spatial/point-pattern-analysis-in-r.html#computing-a-pseudo-p-value) for more information.\n\n\n::: {.cell hash='Homogenous_ANN_cache/html/unnamed-chunk-6_8c5a05d9b3fd56586426acc0a3d05eab'}\n\n```{.r .cell-code}\nN.greater <- sum(ann.r > ann.p)\np <- min(N.greater + 1, n + 1 - N.greater) / (n + 1)\np\n```\n\n[1] 0.3115\n:::\n\n\nAbout 31% of the simulated ANN value were more extreme than our obserbved ANN value of 13.29. Hence, the p-value suggests that we would be 31% wrong in rejecting the null hypothesis that a random process *could* have generated a pattern that is *more dispersed* than our observed pattern.\n\n::: {style=\"background:#FFE4C4;\"}\nNOTES:\n\n-   If you are familiar with the concepts of a one-sided and two-sided test, you could double the p-value and state that *\"... there is a 62% chance of being wrong in rejecting the null hypothesis that a random process could have generated a point pattern similar to our observed pattern\"*. Note the lack of reference to **greater than** or **less than**.\n\n-   Just because our hypothesis test suggests that our observed ANN value is consistent with a random process does not imply that a random process was **the** process behind the distribution of Walmart stores (in fact, it's quite doubtful that Walmart executives assign store location at random). All that a hypothesis test can do is state whether a hypothesized process could be one of **many** other processes that generated the pattern observed in our dataset.\n:::\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}