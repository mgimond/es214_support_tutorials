{"title":"ANN Hypothesis Testing: homogeneous case","markdown":{"yaml":{"title":"ANN Hypothesis Testing: homogeneous case","author":"Manny Gimond"},"headingText":"Load and prep the dataset","containsRefs":false,"markdown":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE, message = FALSE, warning=FALSE, results=\"hide\", tidy=FALSE )\n```\n\n------------------------------------------------------------------------\n\n[Data for this tutorial can be downloaded [from here](./walmarts.zip). Don't forget to unzip the files to a dedicated folder on your computer.]{.block1}\n\n-----\n\n> In RStudio, don't forget to set the R session to the project folder via *Session \\>\\> Set Working Directory \\>\\> Choose Directory*.\n\n-----\n\n\nThe following chunk of code loads the shapefiles into R. They are then converted to  spatial formats readable by the `spatstat` functions. We will also convert the mapping units from meters to kilometers using the `rescale` function. This last step will generate distance values of kilometers instead of meters when we compute the average nearest neighbor value. Much like ArcGIS, R adopts the layer's coordinate system's map units when expressing distance or area values, so by changing the layer's default map units, we end up with output distance values that are no more than 3 or 4 digits long and easier to \"interpret\". \n\n\n```{r, message=FALSE, results='hide', message=FALSE, warning=FALSE}\n# Load packages\nlibrary(sf)\nlibrary(spatstat)\n\n# Read state polygon data\ns  <- st_read(\"MA.shp\")\nw  <- as.owin(s)\nw.km <- rescale(w, 1000)  # rescale map units to km\n\n# Read Walmart point data\ns  <- st_read(\"Walmarts.shp\")\np  <- as.ppp(s)\nmarks(p) <- NULL  # Remove attribute table (simplifies plot operations)\np.km <- rescale(p, 1000) # Rescale map units to km\nWindow(p.km) <- w.km      \n```\n\n## Average nearest neighbor analysis\n\nFirst, we'll compute the observed Walmart ANN statistic.\n\n```{r}\nann.p <- mean(nndist(p.km, k=1))\nann.p\n```\n\nThe observed average nearest neighbor is `r round(ann.p, 2)` km.\n\n### Is our observed ANN value consistent with a random process?\n\nIn this hypothesis test, we are hypothesizing that the process that generated the observed distribution of Walmart stores was completely random. This is our **null hypothesis**. We'll therefore compare our observed ANN value to the *range* of ANN values we could expect to get *if* the Walmart stores were randomly distributed. This will involve randomly shuffling the Walmart store locations, then computing the average distance between the randomly distributed stores. This process is repeated many times such that a distribution of ANN values under the assumption of complete randomness (the null hypothesis) is generated.\n\n```{r}\nn     <- 1999       # Number of times to run the simulation\nann.r <- vector()   # Create an empty object to be used to store the simulated  ANN values\n\nfor (i in 1:n){\n  rand.p   <- rpoint(n = p.km$n, win = w.km)  # Generate random point locations\n  ann.r[i] <- mean(nndist(rand.p, k = 1))     # Compute and store the simulated ANN value\n}\n```\n\nIn the above loop, the function `rpoint` is passed two parameters: `n = p.km$n` and `win = w.km`. The first tells the function how many points to randomly generate (i.e. the same number of points as that in the Walmart points layer which we can extract from the `p.km$n` object). The second tells the function to confine the randomly generated points to the extent defined by `w.km` (the `MA` polygon).\n\nNote that after running the last simulation, you can view its set of randomly generated points via:\n\n```{r fig.height=1.5, echo=2}\nOP <- par(mar = c(0,0,0,0))\nplot(rand.p, pch = 16, main = NULL)\npar(OP)\n```\n\nGiven that this is a random process, your output will look different--as expected.\n\nNext, we plot the histogram of the simulated ANN values then add a blue line showing where our observed ANN value lies relative to the distribution of simulated ANN values under the null. (Your histogram may look different given the random nature of the simulation).\n\n```{r, fig.height=2, echo=2:3}\nOP <- par(mar=c(4,4,0,0))\nhist(ann.r, breaks = 40, col = \"bisque\", xlim = range(ann.p, ann.r), main = NULL)\nabline(v = ann.p, col = \"blue\", lw = 2)  # lw = 2 increases the line width\npar(OP)\n```\n\nThe test suggests that our observed ANN value may not be that different from the `r n` ANN values we simulated under the assumption that the stores are randomly distributed. Our observed values is a tad bit to the right of the center of the distribution suggesting that our observed ANN value might be on the dispersed side of the range of values (a larger than expected ANN value suggests a more dispersed set of points, and a smaller than expected ANN value suggests a more clustered set of points).\n\n### Extracting a p-value from the simulation\n\nWe first need to find the end of the distribution that is *closest* to the observed ANN value. We then find the number of simulated ANN values more extreme than our observed ANN value. Finally, we divide that count by the total number of simulations. Note that this is a so-called *one-sided* P-value. See [lecture notes](https://mgimond.github.io/Spatial/hypothesis-testing.html#a-better-approach-a-monte-carlo-test) for more information.\n\n```{r results = 'asis'}\nN.greater <- sum(ann.r > ann.p)\np <- min(N.greater + 1, n + 1 - N.greater) / (n + 1)\np\n```\n\nAbout `r round(p, 2) * 100`% of the simulated ANN value were more extreme than our obserbved ANN value of `r round(ann.p, 2)`. Hence, the p-value suggests that we would be `r round(p, 2) * 100`% wrong in rejecting the null hypothesis that a random process *could* have generated a pattern that is *more dispersed* than our observed pattern.\n\n::: {style=\"background:#FFE4C4;\"}\nNOTES:\n\n-   If you are familiar with the concepts of a one-sided and two-sided test, you could double the p-value and state that *\"... there is a `r round(p, 2) * 2 * 100`% chance of being wrong in rejecting the null hypothesis that a random process could have generated a point pattern similar to our observed pattern\"*. Note the lack of reference to **greater than** or **less than**.\n\n-   Just because our hypothesis test suggests that our observed ANN value is consistent with a random process does not imply that a random process was **the** process behind the distribution of Walmart stores (in fact, it's quite doubtful that Walmart executives assign store location at random). All that a hypothesis test can do is state whether a hypothesized process could be one of **many** other processes that generated the pattern observed in our dataset.\n:::\n\n\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"output-file":"Homogenous_ANN.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.189","theme":["cosmo","../custom.scss"],"title":"ANN Hypothesis Testing: homogeneous case","author":"Manny Gimond"},"extensions":{"book":{"multiFile":true}}}}}