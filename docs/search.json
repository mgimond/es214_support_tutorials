[
  {
    "objectID": "Getting_started_with_R/Getting_started_with_R.html",
    "href": "Getting_started_with_R/Getting_started_with_R.html",
    "title": "Loading and visualizing data in R",
    "section": "",
    "text": "Data for this tutorial can be downloaded from here. Don’t forget to unzip the files to a dedicated folder on your computer.\nR is a data analysis environment. RStudio is a desktop interface to R (sometimes referred to as an integrated development environment-or IDE for short). Unlike most desktop environments you have been exposed to so far, R does not take instructions from a point-and-click environment, its instructions are provided by simple lines of text."
  },
  {
    "objectID": "Getting_started_with_R/Getting_started_with_R.html#installing-packages",
    "href": "Getting_started_with_R/Getting_started_with_R.html#installing-packages",
    "title": "Loading and visualizing data in R",
    "section": "Installing packages",
    "text": "Installing packages\nThis R session will make use of five packages: spatstat which has all of the spatial statistics tools used in this exercise; sf which is used to load the vector files; maptools which converts the raster format to an ‘im’ format recognized by spatstat; and raster which is used to manipulate raster files. You’ll also need to install rgdal to help the raster package read the .img raster file.\nYou can install packages in one of two ways: via command line or via the RStudio interface.\n\nOption 1: Command line\n\ninstall.packages(\"sf\")\ninstall.packages(\"spatstat\")\ninstall.packages(\"raster\")\ninstall.packages(\"rgdal\")\ninstall.packages(\"maptools\")\n\n\n\nOption 2: RStudio interface"
  },
  {
    "objectID": "Getting_started_with_R/Getting_started_with_R.html#opening-a-new-r-script",
    "href": "Getting_started_with_R/Getting_started_with_R.html#opening-a-new-r-script",
    "title": "Loading and visualizing data in R",
    "section": "Opening a new R script",
    "text": "Opening a new R script\nIf an empty R script file is not opened in RStudio open a new one now.\n\nR scripts are usually saved using the .R extension (e.g. Day14.R). Make sure to save this script on a regular basis as you add/modify pieces of code."
  },
  {
    "objectID": "Getting_started_with_R/Getting_started_with_R.html#setting-an-r-sessions-workspace",
    "href": "Getting_started_with_R/Getting_started_with_R.html#setting-an-r-sessions-workspace",
    "title": "Loading and visualizing data in R",
    "section": "Setting an R session’s workspace",
    "text": "Setting an R session’s workspace\nIf you plan to read or write files from/to a directory, you might find it beneficial to explicitly define the R session’s project folder. To set a session’s working directory, go to Session >> Set Working Directory >> Choose Directory. In this example, you will want to set the working directory to the folder that houses the in-class dataset downloaded from filer (e.g. the walmart/ folder)."
  },
  {
    "objectID": "Getting_started_with_R/Getting_started_with_R.html#loading-activating-packages",
    "href": "Getting_started_with_R/Getting_started_with_R.html#loading-activating-packages",
    "title": "Loading and visualizing data in R",
    "section": "Loading (activating) packages",
    "text": "Loading (activating) packages\nInstalling packages under your user profile is a one-time process, but to access the package contents in a current R session you must explicitly load its contents via the library function.\n\nlibrary(sf)\nlibrary(spatstat)\nlibrary(raster)\nlibrary(maptools) \n\nNote that you do not need to load the rgdal package since its functionality is used in the raster package."
  },
  {
    "objectID": "Getting_started_with_R/Getting_started_with_R.html#loading-shapefiles",
    "href": "Getting_started_with_R/Getting_started_with_R.html#loading-shapefiles",
    "title": "Loading and visualizing data in R",
    "section": "Loading shapefiles",
    "text": "Loading shapefiles\nNote that R will recognize vector data models stored as shapefiles, but it will not recognize GIS files stored in geodatabases.\nFirst, we will load the Massachusetts polygon shapefile into R and save the contents of that shapefile in an object called s2. Note the use of the assignment operator <- which assigns the output to its right to the object to its left. The name of the shapefile must end with the *.shp extension, but note that the function understands that the shapefile consists of multiple files.\n\ns2 <- st_read(\"MA.shp\")\n\nReading layer `MA' from data source \n  `C:\\Users\\mgimond\\Documents\\github\\test_quarto\\Getting_started_with_R\\MA.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1 feature and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 623157.2 ymin: 4577879 xmax: 922141 ymax: 4756659\nProjected CRS: NAD83 / UTM zone 18N\n\n\n\nNOTE: if you get the error message Error: Cannot open \"MA.shp\"; The file doesn't seem to exist., then you probably did not properly set the working directory in an earlier step or you have a syntax error in the filename.\n\nR can store spatial objects in different internal formats but spatstat’s functions require that specific spatial formats be used. The MA states layer will be used to define the study extent which will require that it be stored as a owin object. We will make use of the as.owin function to convert the s2 object to an owin object.\n\nw  <- as.owin(s2)\n\nThe coordinate unit associated with the spatial object inherits the underlying coordinate system’s map units–meters in our example. Such small units may make it difficult to interpret the output of some analyses (e.g. distance based and density based analyses). We will therefore convert the map units from meters to kilometers using the rescale() function. Note that 1000 m = 1 km.\n\nw.km <- rescale(w, 1000)\n\nThe second parameter in the rescale() function, 1000, tells R to divide the current measure of unit by 1000.\nNext we will load the Walmarts stores shapefile (Walmarts.shp) using the same functions, but instead of storing the shapefile as a polygon boundary, we will convert the point shapefile to a ppp point object.\n\ns1 <- st_read(\"Walmarts.shp\")  \n\nReading layer `Walmarts' from data source \n  `C:\\Users\\mgimond\\Documents\\github\\test_quarto\\Getting_started_with_R\\Walmarts.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 44 features and 40 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 640627.2 ymin: 4614803 xmax: 869353.2 ymax: 4738331\nProjected CRS: NAD83 / UTM zone 18N\n\np  <- as.ppp(s1)  # creates a ppp object\n\nWarning in as.ppp.sf(s1): only first attribute column is used for marks\n\np.km <- rescale(p, 1000)\n\nWe will need to explicitly define the study extent for the point object. This will be critical when running the point process models.\n\nWindow(p.km) <- w.km\n\nThere is one more thing that we will need to do that will make the data behave with spatstats tools: remove the layer’s attribute information (point attributes are also known as marks in the point pattern analysis world). The point attributes will not be needed here since our interest is in the pattern generated by the points and not by their attribute values.\n\nmarks(p.km) <- NULL"
  },
  {
    "objectID": "Getting_started_with_R/Getting_started_with_R.html#loading-raster-files",
    "href": "Getting_started_with_R/Getting_started_with_R.html#loading-raster-files",
    "title": "Loading and visualizing data in R",
    "section": "Loading Raster Files",
    "text": "Loading Raster Files\nNext, we will load the population density raster file pop_sqmile.img using the raster function from the raster package, convert it to an im object recognized by spatstat and name the object r.km once re-scaled.\n\nimg  <- raster(\"./pop_sqmile.img\") # Creates a raster object\nr    <- as.im(img)  # Convert r object to an im object\nr.km <- rescale(r, 1000)"
  },
  {
    "objectID": "Homogenous_ANN/Homogenous_ANN.html",
    "href": "Homogenous_ANN/Homogenous_ANN.html",
    "title": "ANN Hypothesis Testing: homogeneous case",
    "section": "",
    "text": "Data for this tutorial can be downloaded from here. Don’t forget to unzip the files to a dedicated folder on your computer."
  },
  {
    "objectID": "Homogenous_ANN/Homogenous_ANN.html#load-and-prep-the-dataset",
    "href": "Homogenous_ANN/Homogenous_ANN.html#load-and-prep-the-dataset",
    "title": "ANN Hypothesis Testing: homogeneous case",
    "section": "Load and prep the dataset",
    "text": "Load and prep the dataset\nIn the following chunk of code, we will load the shapefiles into R, then we will convert the spatial objects into formats that are readable by the spatstat functions. We will also convert the mapping units from meters to kilometers using the rescale function. This last step will generate distance values of kilometers instead of meters when we compute the average nearest neighbor value. Much like ArcGIS, R will adopt the layer’s coordinate system’s map units when expressing distance or area values, so by changing the layer’s default map units, we end up with output distance values that are no more than 3 or 4 digits long and easier to “interpret”.\n\n# Load packages\nlibrary(sf)\nlibrary(spatstat)\n\n# Read state polygon data\ns  <- st_read(\"MA.shp\")\nw  <- as.owin(s)\nw.km <- rescale(w, 1000)  # rescale map units to km\n\n# Read Walmart point data\ns  <- st_read(\"Walmarts.shp\")\np  <- as.ppp(s)\n\nWarning in as.ppp.sf(s): only first attribute column is used for marks\n\nmarks(p) <- NULL  # Remove attribute table (simplifies plot operations)\np.km <- rescale(p, 1000) # Rescale map units to km\nWindow(p.km) <- w.km"
  },
  {
    "objectID": "Homogenous_ANN/Homogenous_ANN.html#average-nearest-neighbor-analysis",
    "href": "Homogenous_ANN/Homogenous_ANN.html#average-nearest-neighbor-analysis",
    "title": "ANN Hypothesis Testing: homogeneous case",
    "section": "Average nearest neighbor analysis",
    "text": "Average nearest neighbor analysis\nFirst, we’ll compute the observed Walmart ANN statistic.\n\nann.p <- mean(nndist(p.km, k=1))\nann.p\n\n[1] 13.29478\n\n\nThe observed average nearest neighbor is 13.29 km.\n\nIs our observed ANN value consistent with a random process?\nIn this hypothesis test, we are hypothesizing that the process that generated the observed distribution of Walmart stores was completely random. This is our null hypothesis. We’ll therefore compare our observed ANN value to the range of ANN values we could expect to get if the Walmart stores were randomly distributed. This will involve randomly shuffling the Walmart store locations, then computing the average distance between the randomly distributed stores. This process is then repeated many times such that a distribution of ANN values under the assumption of complete randomness (the null hypothesis) is generated.\n\nn     <- 1999       # Number of times to run the simulation\nann.r <- vector()   # Create an empty object to be used to store the simulated  ANN values\n\nfor (i in 1:n){\n  rand.p   <- rpoint(n = p.km$n, win = w.km)  # Generate random point locations\n  ann.r[i] <- mean(nndist(rand.p, k = 1))     # Compute and store the simulated ANN value\n}\n\nIn the above loop, the function rpoint is passed two parameters: n = p.km$n and win = w.km. The first tells the function how many points to randomly place (i.e. the same number of points as that in the Walmart points layer which we can extract from the p.km$n object). The second tells the function to confine the randomly generated points to the extent defined by w.km (the MA polygon).\nNote that after running the last simulation, you can view its set of randomly generated points via:\n\nplot(rand.p, pch = 16, main = NULL)\n\n\n\n\nGiven that this is a random process, your output will look different–as expected.\nNext, let’s plot the histogram of the simulated ANN values then add a blue line showing where our observed ANN value lies relative to the distribution of simulated ANN values under the null. (Your histogram may look different given the random nature of the simulation).\n\nhist(ann.r, breaks = 40, col = \"bisque\", xlim = range(ann.p, ann.r), main = NULL)\nabline(v = ann.p, col = \"blue\", lw = 2)  # lw = 2 increases the line width\n\n\n\n\nThe test suggests that our observed ANN value may not be that different from the 1999 ANN values we simulated under the assumption that the stores are randomly distributed. Our observed values is a tad bit to the right of the center of the distribution suggesting that our observed ANN value might be on the dispersed side of the range of values (a larger than expected ANN value suggests a more dispersed set of points, and a smaller than expected ANN value suggests a more clustered set of points).\n\n\nExtracting a p-value from the simulation\nWe first need to find the end of the distribution that is closest to the observed ANN value. We then find the number of simulated ANN values more extreme than our observed ANN value. Finally, we divide that count by the total number of simulations. Note that this is a so-called one-sided P-value. See lecture notes for more information.\n\nN.greater <- sum(ann.r > ann.p)\np <- min(N.greater + 1, n + 1 - N.greater) / (n + 1)\np\n[1] 0.3165\n\nAbout 32% of the simulated ANN value were more extreme than our obserbved ANN value of 13.29. Hence, the p-value suggests that we would be 32% wrong in rejecting the null hypothesis that a random process could have generated a pattern that is more dispersed than our observed pattern.\n\nNOTES:\n\nIf you are familiar with the concepts of a one-sided and two-sided test, you could double the p-value and state that “… there is a 64% chance of being wrong in rejecting the null hypothesis that a random process could have generated a point pattern similar to our observed pattern”. Note the lack of reference to greater than or less than.\nJust because our hypothesis test suggests that our observed ANN value is consistent with a random process does not imply that a random process was the process behind the distribution of Walmart stores (in fact, it’s quite doubtful that Walmart executives assign store location at random). All that a hypothesis test can do is state whether a hypothesized process could be one of many other processes that generated the pattern observed in our dataset."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "This repo houses standalone R scripts used in Colby’s ES214 course",
    "section": "",
    "text": "Loading and visualizing spatial data in R\nPoisson point process model\nANN Hypothesis Testing: homogeneous case\nANN Hypothesis Testing: inhomogeneous case"
  },
  {
    "objectID": "Inhomogenous_ANN/Inhomogenous_ANN.html",
    "href": "Inhomogenous_ANN/Inhomogenous_ANN.html",
    "title": "ANN Hypothesis Testing: inhomogeneous case",
    "section": "",
    "text": "Data for this tutorial can be downloaded from here. Don’t forget to unzip the files to a dedicated folder on your computer."
  },
  {
    "objectID": "Inhomogenous_ANN/Inhomogenous_ANN.html#load-and-prep-the-dataset",
    "href": "Inhomogenous_ANN/Inhomogenous_ANN.html#load-and-prep-the-dataset",
    "title": "ANN Hypothesis Testing: inhomogeneous case",
    "section": "Load and prep the dataset",
    "text": "Load and prep the dataset\nIn the following chunk of code, we will load the shapefiles into R, then we will convert the spatial objects into formats that are readable by the spatstat functions. We will also convert the mapping units from meters to kilometers using the rescale function.\nAs was noted in our PPM analysis of the population data, given its strongly skewed distribution we found it best to express it on a log scale as opposed to a linear scale. We’ll therefore adopt this log scale in this analysis too.\n\n# Load packages\nlibrary(sf)\nlibrary(maptools)\nlibrary(spatstat)\nlibrary(raster)\n\n# Read state polygon data\ns  <- st_read(\"MA.shp\")\nw  <- as.owin(s)\nw.km <- rescale(w, 1000)\n\n# Read Walmart point data\ns  <- st_read(\"Walmarts.shp\")\np  <- as.ppp(s)\n\nWarning in as.ppp.sf(s): only first attribute column is used for marks\n\nmarks(p) <- NULL\np.km <- rescale(p, 1000)\nWindow(p.km) <- w.km     \n\n# Read population raster\nimg     <- raster(\"./pop_sqmile.img\") # Creates a raster object\npop     <- as.im(img)  # Convert r object to an im object\npop.km  <- rescale(pop, 1000)\npop.km.log <- log(pop.km)  # Log transform population data"
  },
  {
    "objectID": "Inhomogenous_ANN/Inhomogenous_ANN.html#average-nearest-neighbor-analysis",
    "href": "Inhomogenous_ANN/Inhomogenous_ANN.html#average-nearest-neighbor-analysis",
    "title": "ANN Hypothesis Testing: inhomogeneous case",
    "section": "Average nearest neighbor analysis",
    "text": "Average nearest neighbor analysis\nFirst, we’ll compute the observed Walmart ANN statistic.\n\nann.p <- mean(nndist(p.km, k=1))\nann.p\n\n[1] 13.29478\n\n\nThe observed average nearest neighbor is 13.29 km.\n\nIs our observed ANN value consistent with a random process when controlled for population distribution (inhomogeneous process)?\nIn our earlier analysis of the point pattern, we calculated a (pseudo) p-value of around 0.33. The problem with our initial ANN analysis is that we did not account for 1st order effects of the underlying process such as population distribution (a possible covariate). In other words, is the distance observed between Walmart stores a reflection of the attractive/repulsive forces at play when positioning the stores within the state of Massachusetts or is their proximity to one another dictated by some underlying process such as population density distribution?\nIf we are to assume that population distribution will influence the distribution of Walmart stores, we need to rerun the ANN analysis while controlling for population distribution influence. We do this by instructing the rpoint() function to increase point placement probability at locations with high covariate values (i.e. a high population density area is more likely to receive a random point than a low density area).\n\nn     <- 999\nann.het <- vector()\nfor (i in 1:n){\n  rand.p   <- rpoint(n = p.km$n, f = pop.km.log, win = w.km) \n  ann.het[i] <- mean(nndist(rand.p, k = 1))\n}\n\nThe above loop is almost identical to that of the homogeneous case except with the addition of the f = pop.km.log argument which defines the intensity of the underlying process (pop.km.log represents the population density raster). The rpoint function rescales the raster values to a range of [0,1] where 1 designates maximum probability of a pixel receiving a point and 0 minimum (or no) probability of a pixel receiving a point. Note that this is still a random process in that each time the rpoint function is run, we’ll have a different point pattern. However, on average, more points will be located where the pop.km.log pixels have the highest values.\nNote that the f = parameter will supersede that of the win = parameter. In other words, if pop.km.log covers a different extent than w.km, the rpoint() function will generate random points within the pop.km.log extent instead of the w.km extent.\nThe following map shows an example of how the Walmart store distribution could look like if dictated by population alone.\n\nplot(rand.p, pch = 16, main = NULL)\n\n\n\n\nNow let’s plot the histogram of simulated ANN values:\n\nhist(ann.het, breaks = 40, col = \"bisque\", xlim = range(ann.p, ann.het), main = NULL)\nabline(v = ann.p, col = \"blue\", lw = 2)\n\n\n\n\nThe histogram is displaying the range of expected ANN values when the placement of Walmart points are dictated by the population distribution.\nOur observed ANN value lies to the right of the distribution center suggesting that our observed Walmart may be more dispersed than expected under the current hypothesis. This makes sense since you would expect there to be a minimum distance between stores to avoid overlapping markets.\n\nN.greater <- sum(ann.het > ann.p)\np <- min(N.greater + 1, n + 1 - N.greater) / (n +1)\np\n\n[1] 0.148"
  },
  {
    "objectID": "Inhomogenous_ANN/Inhomogenous_ANN.html#final-note",
    "href": "Inhomogenous_ANN/Inhomogenous_ANN.html#final-note",
    "title": "ANN Hypothesis Testing: inhomogeneous case",
    "section": "Final note",
    "text": "Final note\nWhile the wording in the last paragraph may imply that it’s the observed ANN value that has shifted along the x-axis relative to the hypothesized distribution, it’s really the distribution that gets shifted along the x-axis. The following plot overlays the expected ANN distribution given a homogeneous 1st order process in red and the expected ANN distribution given the inhomogeneous 1st order process in green. The observed ANN value of 13.3 km is constant under both analyses.\n\n\n\n\nn     <- 1999       \nann.hom <- vector()   \n\nfor (i in 1:n){\n  rand.p   <- rpoint(n = p.km$n, win = w.km)  \n  ann.hom[i] <- mean(nndist(rand.p, k = 1))    \n}\n\nh1 <- hist(ann.hom, breaks=40, col = \"bisque\",  \n           main = NULL, plot = FALSE)\n\nWarning in hist.default(ann.hom, breaks = 40, col = \"bisque\", main = NULL, :\narguments 'col', 'main' are not made use of\n\nh2 <- hist(ann.het, breaks=40, col = \"green\",  \n           main = NULL, plot = FALSE)\n\nWarning in hist.default(ann.het, breaks = 40, col = \"green\", main = NULL, :\narguments 'col', 'main' are not made use of\n\nplot(h1,  xlim=range(c(ann.hom, ann.het, ann.het)),\n     main = \"Homogeneous (red) vs Inhomogeneous (Green)\",\n     col = rgb(1,0,0,0.5), border = \"red\",\n     xlab = \"ANN (km)\")\nplot(h2, col = rgb(0,1,0,0.5), add = TRUE, border = \"green\")\nabline(v = ann.p, col = \"blue\", lw = 3)"
  },
  {
    "objectID": "Inhomogenous_ANN/Inhomogenous_ANN.html#addendum",
    "href": "Inhomogenous_ANN/Inhomogenous_ANN.html#addendum",
    "title": "ANN Hypothesis Testing: inhomogeneous case",
    "section": "Addendum",
    "text": "Addendum\nIt’s good practice to indicate, in your analysis results, if you use a transformed version of the covariate since this can impact the outcome of the analysis. In the following example, the original linear version of the population raster is used.\n\nann.r <- vector()\nfor (i in 1:n){\n  rand.p   <- rpoint(n = p.km$n, f = pop.km, win = w.km) \n  ann.r[i] <- mean(nndist(rand.p, k = 1))\n}\n\n\nhist(ann.r, breaks = 40, col = \"bisque\", xlim = range(ann.p, ann.r), main = NULL)\nabline(v = ann.p, col = \"blue\", lw = 2)\n\n\n\n\n\nN.greater <- sum(ann.r > ann.p)\np <- min(N.greater + 1, n + 1 - N.greater) / (n + 1)\np\n\n[1] 0.0015\n\n\nThe outcome of this analysis is quite different from the one using the raw population density values. Both outcomes are perfectly valid, but they address slightly different questions: “When controlling for the population density distribution, are Walmart stores randomly distributed?” vs. “When controlling for the population density distribution as measured on a log scale, are Walmart stores randomly distributed?”"
  },
  {
    "objectID": "PPM/PPM_analysis.html",
    "href": "PPM/PPM_analysis.html",
    "title": "PPM: Exploring 1st order effects",
    "section": "",
    "text": "Data for this tutorial can be downloaded from here. Don’t forget to unzip the files to a dedicated folder on your computer."
  },
  {
    "objectID": "PPM/PPM_analysis.html#loading-the-data-into-r",
    "href": "PPM/PPM_analysis.html#loading-the-data-into-r",
    "title": "PPM: Exploring 1st order effects",
    "section": "Loading the data into R",
    "text": "Loading the data into R\nAlways remember to set the R session to the project folder where data are to be read from.\n\n# Load packages\n# Note that rgdal should also be installed on your computer\n# even though it's not explicitly called in the library function\nlibrary(spatstat)\nlibrary(maptools)\nlibrary(sf)\nlibrary(raster)\n\n# Read state polygon data\ns    <- st_read(\"MA.shp\")\nw    <- as.owin(s)\nw.km <- rescale(w, 1000)\n\n# Read Walmart point data\ns    <- st_read(\"Walmarts.shp\")\np    <- as.ppp(s)\n\nWarning in as.ppp.sf(s): only first attribute column is used for marks\n\np.km <- rescale(p, 1000)\nmarks(p.km)  <- NULL\nWindow(p.km) <- w.km\n\n# Read population density raster\nimg         <- raster(\"pop_sqmile.tif\")\npop         <- as.im(img)\npop.km      <- rescale(pop, 1000)\npop.km.log  <- log(pop.km)\n\n# Read median income raster\nimg    <- raster(\"median_income.tif\")\ninc    <- as.im(img)\ninc.km <- rescale(inc, 1000)\ninc.km[] <- as.double(inc.km[]) # Convert integer income values to double values\n\nYou’ll note that we are converting the income raster from integer data type to double (inc[] <- as.double(inc[])). This is done to satisfy the effectfun() function that will be used later in this workflow–that function does not accept integer rasters.\nYou’ll also note in the above chunk of code that we are transforming population density values to logged values (log(pop.km)) because of the skewed nature of the population raster. This may help improve the model performance. Such transformation techniques are not uncommon in the field of statistics.\nA quick way to check the distribution of raster pixel values is to plot the histogram as follows:\n\n# Plot original raster values\nhist(pop.km)\n\n\n\n\n\n# Plot log-transformed raster values\nhist(pop.km.log)\n\n\n\n\nLet’s plot each raster with the Walmart point overlay. We’ll use R’s base plotting environment.\n\nplot(pop.km.log, ribside=\"bottom\", main=\"Population\")\nplot(p.km, pch = 20, col=rgb(1,1,1,0.5), add=TRUE)\n\nplot(inc.km, ribside=\"bottom\", main=\"Income\")\nplot(p.km, pch = 20, col=rgb(1,1,1,0.6), add=TRUE)"
  },
  {
    "objectID": "PPM/PPM_analysis.html#modeling-point-density-as-a-function-of-two-competing-covariates-population-density-and-income.",
    "href": "PPM/PPM_analysis.html#modeling-point-density-as-a-function-of-two-competing-covariates-population-density-and-income.",
    "title": "PPM: Exploring 1st order effects",
    "section": "Modeling point density as a function of two competing covariates: population density and income.",
    "text": "Modeling point density as a function of two competing covariates: population density and income.\nWe will first develop two different models that we think might define the Walmart’s point intensity. These models are defined by a couple of covariates: population distribution and income. These will be the alternate models that we’ll denote as Mpop and Minc. The models’ structure will follow the form of a logistics model, but note that the models can take on many different forms and different levels of complexity.\nWe’ll also create the null model, Mo, where we’ll assume a spatially uniform (homogeneous) covariate. In other words, Mo will define the model where we the intensity of the process is the same across the entire study extent.\n\nMpop <- ppm(p.km ~ pop.km.log) # Population model\n\nWarning: Values of the covariate 'pop.km.log' were NA or undefined at 3.4% (17\nout of 504) of the quadrature points. Occurred while executing: ppm.ppp(Q =\np.km, trend = ~pop.km.log, data = NULL, interaction = NULL)\n\nMinc <- ppm(p.km ~ inc.km)     # Income model\n\nWarning: Values of the covariate 'inc.km' were NA or undefined at 3.2% (16 out\nof 504) of the quadrature points. Occurred while executing: ppm.ppp(Q = p.km,\ntrend = ~inc.km, data = NULL, interaction = NULL)\n\nMo   <- ppm(p.km ~ 1)          # Null model\n\nLet’s explore the model parameters. First, we’ll look at Mpop.\n\nMpop\n\nNonstationary Poisson process\n\nLog intensity:  ~pop.km.log\n\nFitted trend coefficients:\n(Intercept)  pop.km.log \n -10.111647    0.625803 \n\n              Estimate      S.E.     CI95.lo    CI95.hi Ztest       Zval\n(Intercept) -10.111647 0.7933272 -11.6665393 -8.5567540   *** -12.745872\npop.km.log    0.625803 0.1127980   0.4047229  0.8468831   ***   5.547996\nProblem:\n Values of the covariate 'pop.km.log' were NA or undefined at 3.4% (17 out of \n504) of the quadrature points\n\n\nThe values of interest are the intercept (whose value is around -10.1) and the coefficient pop.km.log (whose value is around 0.63). Using these values, we can construct the mathematical relationship (noting that we are using the logged population raster and not the original population raster values):\n\\[\nWalmart\\ intensity(i)= e^{−10.1 + 0.63\\ log(population\\ density_i)}\n\\] The above equation can be interpreted as follows: if the population density is 0, then the Walmart intensity is \\(e^{−10.1}\\) which is very close to 0. So for every unit increase of the logged population density (i.e. log of one person per square mile), there is a \\(e^{0.63}\\) increase in Walmart intensity.\nLikewise, we can extract the parameters from the Minc model and construct its equation.\n\nMinc\n\nNonstationary Poisson process\n\nLog intensity:  ~inc.km\n\nFitted trend coefficients:\n  (Intercept)        inc.km \n-5.576942e+00 -1.654792e-05 \n\n                 Estimate         S.E.       CI95.lo       CI95.hi Ztest\n(Intercept) -5.576942e+00 5.258426e-01 -6.607574e+00 -4.546309e+00   ***\ninc.km      -1.654792e-05 1.490546e-05 -4.576209e-05  1.266624e-05      \n                  Zval\n(Intercept) -10.605725\ninc.km       -1.110192\nProblem:\n Values of the covariate 'inc.km' were NA or undefined at 3.2% (16 out of 504) \nof the quadrature points\n\n\n\\[\nWalmart\\ intensity(i) = e^{−5.58\\ −1.66e^{−5}\\ Income(i)}\n\\] Note the negative (decreasing) relationship between income distribution and Walmart density.\nNext, we’ll extract the null model results:\n\nMo\n\nStationary Poisson process\nIntensity: 0.002086305\n             Estimate      S.E.   CI95.lo   CI95.hi Ztest      Zval\nlog(lambda) -6.172361 0.1507557 -6.467836 -5.876885   *** -40.94281\n\n\nThis gives us the following equation for the homogeneous process:\n\\[\nWalmart\\ intensity(i) = e^{−6.17} = 0.00209\n\\]\nwhich is nothing more than the number of stores per unit area (44 stores / 21,000km2 = 0.00209)."
  },
  {
    "objectID": "PPM/PPM_analysis.html#plotting-the-competing-models",
    "href": "PPM/PPM_analysis.html#plotting-the-competing-models",
    "title": "PPM: Exploring 1st order effects",
    "section": "Plotting the competing models",
    "text": "Plotting the competing models\n\nOP <- par(mfrow = c(1,2), mar = c(4,4,2,1))  # This creates a two-pane plotting window\n\nplot(effectfun(Mpop, \"pop.km.log\", se.fit = TRUE), main = \"Population\",\n     ylab = \"Walmarts per km2\", xlab = \"Population density\", legend = FALSE)\n\nplot(effectfun(Minc, \"inc.km\", se.fit = TRUE), main = \"Income\",\n     ylab = \"Walmarts per km2\", xlab = \"Income\", legend = FALSE)\n\npar(OP) # This reverts our plot window back to a one-pane window\n\n\n\n\nNote the difference in relationships between the two models. In the first plot, we note an increasing relationship between Walmart intensity and population density; this is to be expected since you would not expect to see Walmart stores in underpopulated areas. In the second plot, we note an inverse relationship between Walmart intensity and income—i.e. as an area’s income increases, the Walmart intensity decreases.\nThe grey envelopes encompass the 95% confidence interval; i.e. the true estimate (black line) can fall anywhere within this envelope. Note how the envelope broadens near the upper end of the population density values–this suggests wide uncertainty in the estimated model.\nTo assess how well the above models explain the relationship between covariate and Walmart intensity, we will turn to hypothesis testing."
  },
  {
    "objectID": "PPM/PPM_analysis.html#testing-for-covariate-effect",
    "href": "PPM/PPM_analysis.html#testing-for-covariate-effect",
    "title": "PPM: Exploring 1st order effects",
    "section": "Testing for covariate effect",
    "text": "Testing for covariate effect\nNow, let’s compare the non-homogeneous covariates to the null model using a technique called the likelihood ratio test. Remember that the null model assumes that the intensity is homogeneous across the entire study area; what we want to know is “does the model with the covariate do a significantly better job in predicting Walmart densities than the null model?”\n\nanova(Mo, Mpop, test = \"LRT\") # Compare null to population model\n\nWarning: Values of the covariate 'pop.km.log' were NA or undefined at 3.4% (17\nout of 504) of the quadrature points. Occurred while executing: ppm.ppp(Q =\np.km, trend = ~pop.km.log, data = NULL, interaction = NULL,\n\n\nWarning: Models were re-fitted after discarding quadrature points that were\nillegal under some of the models\n\n\nAnalysis of Deviance Table\n\nModel 1: ~1      Poisson\nModel 2: ~pop.km.log     Poisson\n  Npar Df Deviance  Pr(>Chi)    \n1   18                          \n2   19  1   31.725 1.776e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nanova(Mo, Minc, test = \"LRT\") # Compare null to income model\n\nWarning: Values of the covariate 'inc.km' were NA or undefined at 3.2% (16 out\nof 504) of the quadrature points. Occurred while executing: ppm.ppp(Q = p.km,\ntrend = ~inc.km, data = NULL, interaction = NULL,\n\n\nWarning: Models were re-fitted after discarding quadrature points that were\nillegal under some of the models\n\n\nAnalysis of Deviance Table\n\nModel 1: ~1      Poisson\nModel 2: ~inc.km     Poisson\n  Npar Df Deviance Pr(>Chi)\n1   17                     \n2   18  1   1.3388   0.2473\n\n\nWhat we are seeking is a small p-value (parameter Pr(>Chi) in the output). The smaller the value, the more confident we are in stating that the covariate does a better job in predicting Walmart intensity than the null model. For example, the p-value for the Mpop model (Pr(>Chi) = 1.776e-08) suggests that population density does a better job in predicting Walmart density than the null model Mo.\nThe p-value for Minc, on the other hand, is higher with a value of Pr = 0.247 indicating that there is a 24.7% chance that we would be wrong in stating that income does a better job in predicting Walmart densities. To many, that probability is too high to reject the null.\nSo to summaries: of the two models we tested, it seems that population density does a better job at explaining the distribution of Walmarts (though it’s not perfect) than the null model. Income distribution, on the other hand, does not improve on the null model."
  }
]