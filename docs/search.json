[
  {
    "objectID": "Getting_started_with_R/Getting_started_with_R.html",
    "href": "Getting_started_with_R/Getting_started_with_R.html",
    "title": "Loading and visualizing data in R",
    "section": "",
    "text": "Data for this tutorial can be downloaded from here. Don’t forget to unzip the files to a dedicated folder on your computer.\nR is a data analysis environment. RStudio is a desktop interface to R (sometimes referred to as an integrated development environment-or IDE for short). Unlike most desktop environments you have been exposed to so far, R does not take instructions from a point-and-click environment, its instructions are provided by simple lines of text."
  },
  {
    "objectID": "Getting_started_with_R/Getting_started_with_R.html#installing-packages",
    "href": "Getting_started_with_R/Getting_started_with_R.html#installing-packages",
    "title": "Loading and visualizing data in R",
    "section": "Installing packages",
    "text": "Installing packages\nThis course will make use of four packages: sf which is used to load, manipulate and visualize vector files; terra which is used to load, manipulate and visualize raster files; spatstat which will be used for point pattern analysis; spdep which will be used to analyze spatial autocorrelation in the data.\nYou can install packages in one of two ways: via command line or via the RStudio interface.\n\nOption 1: Command line\nIf you choose to install the packages via the command line, type the following lines of code in the console, one line at a time.\n\ninstall.packages(\"sf\")\ninstall.packages(\"spatstat\")\ninstall.packages(\"terra\")\ninstall.packages(\"spdep\")\n\n\n\nOption 2: RStudio interface\nIf you choose to install the packages via RStudio’s pull-down menu, click on the Packages &gt;&gt; Install tab and list the packages to be installed in the Packages field separated by commas.\n\n\nIf you are given the option to either have R compile some of the libraries or to accept the binaries, opt for the compilation option."
  },
  {
    "objectID": "Getting_started_with_R/Getting_started_with_R.html#opening-a-new-r-script",
    "href": "Getting_started_with_R/Getting_started_with_R.html#opening-a-new-r-script",
    "title": "Loading and visualizing data in R",
    "section": "Opening a new R script",
    "text": "Opening a new R script\nIf an empty R script file is not open in RStudio open a new one now.\n\nR scripts are usually saved using the .R extension. For this tutorial, you may choose to name the script r_intro.R. Make sure to save this script on a regular basis as you add/modify pieces of code."
  },
  {
    "objectID": "Getting_started_with_R/Getting_started_with_R.html#setting-an-r-sessions-workspace",
    "href": "Getting_started_with_R/Getting_started_with_R.html#setting-an-r-sessions-workspace",
    "title": "Loading and visualizing data in R",
    "section": "Setting an R session’s workspace",
    "text": "Setting an R session’s workspace\nIf you plan to read or write files from/to a directory, you might find it beneficial to explicitly define the R session’s project folder. To set a session’s working directory, go to Session &gt;&gt; Set Working Directory &gt;&gt; Choose Directory. In this example, you will want to set the working directory to the folder that houses the sample dataset."
  },
  {
    "objectID": "Getting_started_with_R/Getting_started_with_R.html#loading-activating-packages",
    "href": "Getting_started_with_R/Getting_started_with_R.html#loading-activating-packages",
    "title": "Loading and visualizing data in R",
    "section": "Loading (activating) packages",
    "text": "Loading (activating) packages\nInstalling packages under your user profile is a one-time process, but to access the package contents in a current R session you must explicitly load its contents via the library function.\nIn this exercise, we will load and prep the data for a point pattern analysis using the spatstat package. We will therefore not need to load the spdep package which won’t be used until we tackle spatial autocorrelation in later exercises.\n\nlibrary(sf)\nlibrary(spatstat)\nlibrary(terra)"
  },
  {
    "objectID": "Getting_started_with_R/Getting_started_with_R.html#loading-shapefiles",
    "href": "Getting_started_with_R/Getting_started_with_R.html#loading-shapefiles",
    "title": "Loading and visualizing data in R",
    "section": "Loading shapefiles",
    "text": "Loading shapefiles\nNote that R will recognize vector data models stored as shapefiles, but it will not recognize GIS files stored in geodatabases.\nFirst, we will load the Massachusetts polygon shapefile into R and save the contents of that shapefile to an object called s2. Note the use of the assignment operator &lt;- which assigns the output to its right to the object to its left. The name of the shapefile must end with the *.shp extension, but note that the function understands that the shapefile consists of multiple files.\n\ns2 &lt;- st_read(\"MA.shp\")\n\nReading layer `MA' from data source \n  `C:\\Users\\mgimond\\Documents\\Github\\es214_support_tutorials\\Getting_started_with_R\\MA.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1 feature and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 623157.2 ymin: 4577879 xmax: 922141 ymax: 4756659\nProjected CRS: NAD83 / UTM zone 18N\n\n\n\nNOTE: if you get the error message Error: Cannot open \"MA.shp\"; The file doesn't seem to exist., then you probably did not properly set the working directory (see earlier step) or you have a syntax error in the file name.\n\nR can store spatial objects in different internal formats. spatstat’s functions require that a specific spatial format be used. The MA states layer will be used to define the study extent. This will require that it be converted to an owin formatted object for use with spatstat. We will make use of the as.owin function to convert the s2 object to an owin object.\n\nw  &lt;- as.owin(s2)\n\nThe coordinate unit associated with the spatial object inherits the underlying coordinate system’s map units–meters in this example. Such small units may make it difficult to interpret the output of some operations given that some analyses may generate very small values (i.e. density based analyses) or very large values (i.e. distance based analyses). We will therefore convert the map units from meters to kilometers using the rescale.owin() function. Note that 1000 m = 1 km.\n\nw.km &lt;- rescale.owin(w, 1000)\n\nThe second parameter in the rescale.owin() function, 1000, tells R to divide the planar unit by 1000.\nNext we will load the Walmart stores shapefile (Walmarts.shp) using the same functions, but instead of storing the shapefile as a polygon boundary, we will convert the point shapefile to a ppp formatted point object.\n\ns1 &lt;- st_read(\"Walmarts.shp\")  \n\nReading layer `Walmarts' from data source \n  `C:\\Users\\mgimond\\Documents\\Github\\es214_support_tutorials\\Getting_started_with_R\\Walmarts.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 44 features and 40 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 640627.2 ymin: 4614803 xmax: 869353.2 ymax: 4738331\nProjected CRS: NAD83 / UTM zone 18N\n\np  &lt;- as.ppp(s1)  # creates a ppp object\n\nWarning in as.ppp.sf(s1): only first attribute column is used for marks\n\np.km &lt;- rescale.ppp(p, 1000)\n\nBy default, R will adopt a rectangular extent for the point data.\n\nGiven that we will be defining the extent to match that of the Massachusetts boundaries, we will need to explicitly define the study extent for the point object.\n\n\nWindow(p.km) &lt;- w.km\n\nNote the uppercase W in the function Window()!\nThere is one more thing that we will need to do that will make the data behave with spatstats tools: remove the layer’s attribute information (attributes are also known as marks in the point pattern analysis world). The point attributes will not be needed here since our interest is in the pattern generated by the points and not by their attribute values.\n\nmarks(p.km) &lt;- NULL"
  },
  {
    "objectID": "Getting_started_with_R/Getting_started_with_R.html#loading-raster-files",
    "href": "Getting_started_with_R/Getting_started_with_R.html#loading-raster-files",
    "title": "Loading and visualizing data in R",
    "section": "Loading Raster Files",
    "text": "Loading Raster Files\nNext, we will load the population density raster file pop_sqmile.img using the rast() function from the terra package, then we’ll convert it to an im object recognized by spatstat and name the object r.km once re-scaled.\n\nimg  &lt;- rast(\"pop_sqmile.img\") # Load raster file\ndf &lt;- as.data.frame(img, xy = TRUE) # Convert to XYZ data table\nr    &lt;- as.im(df)             # Convert XYZ table to im object\nr.km &lt;- rescale.im(r, 1000)"
  },
  {
    "objectID": "Homogenous_ANN/Homogenous_ANN.html",
    "href": "Homogenous_ANN/Homogenous_ANN.html",
    "title": "ANN Hypothesis Testing: homogeneous case",
    "section": "",
    "text": "Data for this tutorial can be downloaded from here. Don’t forget to unzip the files to a dedicated folder on your computer."
  },
  {
    "objectID": "Homogenous_ANN/Homogenous_ANN.html#load-and-prep-the-dataset",
    "href": "Homogenous_ANN/Homogenous_ANN.html#load-and-prep-the-dataset",
    "title": "ANN Hypothesis Testing: homogeneous case",
    "section": "Load and prep the dataset",
    "text": "Load and prep the dataset\nThe following chunk of code loads the shapefiles into R. They are then converted to spatial formats readable by the spatstat functions. We will also convert the mapping units from meters to kilometers using the rescale.* functions. This last step will generate distance values of kilometers instead of meters when we compute the average nearest neighbor value. Much like ArcGIS, R adopts the layer’s coordinate system’s map units when expressing distance or area values, so by changing the layer’s default map units, we end up with output distance values that are no more than 3 or 4 digits long and easier to “interpret”.\n\n# Load packages\nlibrary(sf)\nlibrary(spatstat)\n\n# Read state polygon data\ns  &lt;- st_read(\"MA.shp\")\nw  &lt;- as.owin(s)\nw.km &lt;- rescale.owin(w, 1000)  # rescale map units to km\n\n# Read Walmart point data\ns  &lt;- st_read(\"Walmarts.shp\")\np  &lt;- as.ppp(s)\nmarks(p) &lt;- NULL  # Remove attribute table (simplifies plot operations)\np.km &lt;- rescale.ppp(p, 1000) # Rescale map units to km\nWindow(p.km) &lt;- w.km"
  },
  {
    "objectID": "Homogenous_ANN/Homogenous_ANN.html#average-nearest-neighbor-analysis",
    "href": "Homogenous_ANN/Homogenous_ANN.html#average-nearest-neighbor-analysis",
    "title": "ANN Hypothesis Testing: homogeneous case",
    "section": "Average nearest neighbor analysis",
    "text": "Average nearest neighbor analysis\nFirst, we’ll compute the observed Walmart ANN statistic.\n\nann.p &lt;- mean(nndist(p.km, k=1))\nann.p\n\nThe observed average nearest neighbor is 13.29 km.\n\nIs our observed ANN value consistent with a random process?\nIn this hypothesis test, we are hypothesizing that the process that generated the observed distribution of Walmart stores was completely random. This is our null hypothesis. We’ll therefore compare our observed ANN value to the range of ANN values we could expect to get if the Walmart stores were randomly distributed. This will involve randomly shuffling the Walmart store locations, then computing the average distance between the randomly distributed stores. This process is repeated many times such that a distribution of ANN values under the assumption of complete randomness (the null hypothesis) is generated.\n\nn     &lt;- 1999       # Number of times to run the simulation\nann.r &lt;- vector()   # Create an empty object to be used to store the simulated  ANN values\n\nfor (i in 1:n){\n  rand.p   &lt;- rpoint(n = p.km$n, win = w.km)  # Generate random point locations\n  ann.r[i] &lt;- mean(nndist(rand.p, k = 1))     # Compute and store the simulated ANN value\n}\n\nIn the above loop, the function rpoint is passed two parameters: n = p.km$n and win = w.km. The first tells the function how many points to randomly generate (i.e. the same number of points as that in the Walmart points layer which we can extract from the p.km$n object). The second tells the function to confine the randomly generated points to the extent defined by w.km (the MA polygon).\nNote that after running the last simulation, you can view its set of randomly generated points via:\n\nplot(rand.p, pch = 16, main = NULL)\n\n\n\n\nGiven that this is a random process, your output will look different–as expected.\nNext, we plot the histogram of the simulated ANN values then add a blue line showing where our observed ANN value lies relative to the distribution of simulated ANN values under the null. (Your histogram may look different given the random nature of the simulation).\n\nhist(ann.r, breaks = 40, col = \"bisque\", xlim = range(ann.p, ann.r), main = NULL)\nabline(v = ann.p, col = \"blue\", lw = 2)  # lw = 2 increases the line width\n\n\n\n\nThe test suggests that our observed ANN value may not be that different from the 1999 ANN values we simulated under the assumption that the stores are randomly distributed. Our observed values is a tad bit to the right of the center of the distribution suggesting that our observed ANN value might be on the dispersed side of the range of values (a larger than expected ANN value suggests a more dispersed set of points, and a smaller than expected ANN value suggests a more clustered set of points).\n\n\nExtracting a p-value from the simulation\nWe first need to find the end of the distribution that is closest to the observed ANN value. We then find the number of simulated ANN values more extreme than our observed ANN value. Finally, we divide that count by the total number of simulations. Note that this is a so-called one-sided P-value. See lecture notes for more information.\n\nN.greater &lt;- sum(ann.r &gt; ann.p)\np &lt;- min(N.greater + 1, n + 1 - N.greater) / (n + 1)\np\n[1] 0.3275\n\nAbout 33% of the simulated ANN value were more extreme than our obserbved ANN value of 13.29. Hence, the p-value suggests that we would be 33% wrong in rejecting the null hypothesis that a random process could have generated a pattern that is more dispersed than our observed pattern.\n\nNOTES:\n\nIf you are familiar with the concepts of a one-sided and two-sided test, you could double the p-value and state that “… there is a 66% chance of being wrong in rejecting the null hypothesis that a random process could have generated a point pattern similar to our observed pattern”. Note the lack of reference to greater than or less than.\nJust because our hypothesis test suggests that our observed ANN value is consistent with a random process does not imply that a random process was the process behind the distribution of Walmart stores (in fact, it’s quite doubtful that Walmart executives assign store location at random). All that a hypothesis test can do is state whether a hypothesized process could be one of many other processes that generated the pattern observed in our dataset."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "This repo houses standalone R scripts used in Colby’s ES214 course",
    "section": "",
    "text": "Loading and visualizing spatial data in R"
  },
  {
    "objectID": "index.html#point-pattern-analysis",
    "href": "index.html#point-pattern-analysis",
    "title": "This repo houses standalone R scripts used in Colby’s ES214 course",
    "section": "Point pattern analysis",
    "text": "Point pattern analysis\n\n1st order analysis\n\nPoisson point process model\n\n\n\n2nd order analysis\n\nANN Hypothesis Testing: homogeneous case\nANN Hypothesis Testing: inhomogeneous case"
  },
  {
    "objectID": "index.html#spatial-autocorrelation",
    "href": "index.html#spatial-autocorrelation",
    "title": "This repo houses standalone R scripts used in Colby’s ES214 course",
    "section": "Spatial autocorrelation",
    "text": "Spatial autocorrelation\n\nMoran’s I: Contiguity\nMoran’s I: Distance bands"
  },
  {
    "objectID": "Inhomogenous_ANN/Inhomogenous_ANN.html",
    "href": "Inhomogenous_ANN/Inhomogenous_ANN.html",
    "title": "ANN Hypothesis Testing: Inhomogeneous Case",
    "section": "",
    "text": "Data for this tutorial can be downloaded from here. Don’t forget to unzip the files to a dedicated folder on your computer."
  },
  {
    "objectID": "Inhomogenous_ANN/Inhomogenous_ANN.html#load-and-prep-the-dataset",
    "href": "Inhomogenous_ANN/Inhomogenous_ANN.html#load-and-prep-the-dataset",
    "title": "ANN Hypothesis Testing: Inhomogeneous Case",
    "section": "Load and prep the dataset",
    "text": "Load and prep the dataset\nIn the following chunk of code, we will load the shapefiles into R, then we will convert the spatial objects into formats that are readable by the spatstat functions. We will also convert the mapping units from meters to kilometers using the rescale function.\nOur earlier PPM analysis suggested that the population density distribution (expressed on a log scale), could help explain the observed distribution of Walmart stores. We therefore need to account for this inhomogeneity in the underlying 1st order effect before tackling a distance based analysis which addresses the 2nd order effect of the underlying process. In other words, we want to make sure that any clustering or dispersion observed in our data is not a result of a varying population distribution.\n\n# Load packages\nlibrary(sf)\nlibrary(spatstat)\nlibrary(terra)\n\n# Read state polygon data\ns  &lt;- st_read(\"MA.shp\")\nw  &lt;- as.owin(s)\nw.km &lt;- rescale.owin(w, 1000)\n\n# Read Walmart point data\ns  &lt;- st_read(\"Walmarts.shp\")\np  &lt;- as.ppp(s)\nmarks(p) &lt;- NULL\np.km &lt;- rescale.ppp(p, 1000)\nWindow(p.km) &lt;- w.km     \n\n# Read population raster\nimg      &lt;- rast(\"log_pop_sqmile.tif\") \ndf       &lt;- as.data.frame(img, xy = TRUE)\nrast     &lt;- as.im(df)  \nrast.km  &lt;- rescale.im(rast, 1000)"
  },
  {
    "objectID": "Inhomogenous_ANN/Inhomogenous_ANN.html#average-nearest-neighbor-analysis",
    "href": "Inhomogenous_ANN/Inhomogenous_ANN.html#average-nearest-neighbor-analysis",
    "title": "ANN Hypothesis Testing: Inhomogeneous Case",
    "section": "Average nearest neighbor analysis",
    "text": "Average nearest neighbor analysis\nFirst, we’ll compute the observed Walmart ANN statistic.\n\nann.p &lt;- mean(nndist(p.km, k=1))\nann.p\n\n[1] 13.29478\n\n\nThe observed average nearest neighbor is 13.29 km.\n\nIs our observed ANN value consistent with a random process when controlled for population distribution (inhomogeneous process)?\nIn our earlier analysis of the point pattern, we calculated a (pseudo) p-value of around 0.33. The problem with our initial ANN analysis is that we did not account for 1st order effects of the underlying process such as population distribution (a possible covariate). In other words, is the distance observed between Walmart stores a reflection of the attractive/repulsive forces at play when positioning the stores within the state of Massachusetts or is their proximity to one another dictated by some underlying process such as population density distribution?\nIf we are to assume that population distribution will influence the distribution of Walmart stores, we need to rerun the ANN analysis while controlling for population distribution influence. We do this by instructing the rpoint() function to increase point placement probability at locations with high covariate values (i.e. a high population density area is more likely to receive a random point than a low density area).\n\nn     &lt;- 1999\nann.het &lt;- vector()\nfor (i in 1:n){\n  rand.p   &lt;- rpoint(n = p.km$n, f = rast.km) \n  ann.het[i] &lt;- mean(nndist(rand.p, k = 1))\n}\n\nThe above loop is almost identical to that of the homogeneous case except with the addition of the f = rast.km argument which defines the intensity of the underlying process (rast.km represents the population density raster). The rpoint function rescales the raster values to a range of [0,1] where 1 designates maximum probability of a pixel receiving a point and 0 minimum (or no) probability of a pixel receiving a point. It’s therefore important to remember that you want the large pixel values to be associated with a greater probability of receiving a point. If you want to reverse the relationship, in other words if you want to assign a greater probability of placing a point where you have a small pixel value, simply inverse the pixel values (this can be done in the GIS software or in R via rast.km[] = max(rast.km[]) - rast.km[]. Note that the covariate raster cannot have negative values, hence the values must still be greater than or equal to 0.\nThe following map shows an example of how the Walmart store distribution could look like if dictated by population distribution.\n\nplot(rand.p, pch = 16, main = NULL)\n\n\n\n\nIt may not look terribly different from the homogeneous ANN model, but if you were to tally all point distributions from each simulation, you would very likely find more points occurring in areas with higher population values (as expressed by the raster layer).\nNow, let’s plot the histogram of simulated ANN values:\n\nhist(ann.het, breaks = 40, col = \"bisque\", xlim = range(ann.p, ann.het), main = NULL)\nabline(v = ann.p, col = \"blue\", lw = 2)\n\n\n\n\nThe histogram is displaying the range of expected ANN values when the placement of Walmart points are dictated by the population distribution.\nOur observed ANN value lies to the right of the distribution center suggesting that our observed Walmart may be more dispersed than expected under the current hypothesis. This makes sense since you would expect there to be a minimum distance between stores to avoid overlapping markets.\nThe p-value is thus:\n\nN.greater &lt;- sum(ann.het &gt; ann.p)\np &lt;- min(N.greater + 1, n + 1 - N.greater) / (n +1)\np\n\n[1] 0.1295\n\n\nNote that this is a smaller p-value than that computed under the homogeneous scenario."
  },
  {
    "objectID": "Inhomogenous_ANN/Inhomogenous_ANN.html#final-note",
    "href": "Inhomogenous_ANN/Inhomogenous_ANN.html#final-note",
    "title": "ANN Hypothesis Testing: Inhomogeneous Case",
    "section": "Final note",
    "text": "Final note\nWhile the histogram may imply that it’s the observed ANN value that has shifted rightward along the x-axis relative to the hypothesized distribution, it’s really the distribution that gets shifted along the x-axis. The following plot overlays the expected ANN distribution given a homogeneous 1st order process in red and the expected ANN distribution given the inhomogeneous 1st order process in green. The observed ANN value of 13.3 km is constant under both analyses!\n\n\nWarning in hist.default(ann.hom, breaks = 40, col = \"bisque\", main = NULL, :\narguments 'col', 'main' are not made use of\n\n\nWarning in hist.default(ann.het, breaks = 40, col = \"green\", main = NULL, :\narguments 'col', 'main' are not made use of"
  },
  {
    "objectID": "moranI/Mapping_and_Morans.html",
    "href": "moranI/Mapping_and_Morans.html",
    "title": "Moran’s I analysis in R",
    "section": "",
    "text": "Data for this tutorial can be downloaded from here. Make sure to unzip the files to a dedicated folder on your computer.\nWe will make use of the following packages: sf for importing the shapefiles, tmap for creating choropleth maps and spdep for implementing the Moran’s I analysis. Note that you will probably need to install spdep since this package has not been used thus far in class.\nThe functions in spdep use spatial formats that differ from those used with spatstat. As such, we will no longer need to rely on calls to as.owin(s) and as.ppp(s) as was the case with the spatstat functions.\nlibrary(sf)\nlibrary(spdep)\nlibrary(RColorBrewer)"
  },
  {
    "objectID": "moranI/Mapping_and_Morans.html#loading-the-data",
    "href": "moranI/Mapping_and_Morans.html#loading-the-data",
    "title": "Moran’s I analysis in R",
    "section": "Loading the data",
    "text": "Loading the data\nDon’t forget to set the session’s working directory to the folder that contains the NHME.shp shapefile before running the following chunks of code.\n\ns &lt;- st_read(\"NHME.shp\")\n\nNote that unlike the earlier point pattern analysis exercises, we will need to keep the attribute information with our spatial objects.\nTo list the column names associated with the object’s attribute table, type:\n\nnames(s)\n\n[1] \"NAME\"       \"STATE_NAME\" \"POP10_SQMI\" \"AVE_HH_SZ\"  \"AVE_FAM_SZ\"\n[6] \"Income\"     \"House_year\" \"geometry\"  \n\n\nTo list the contents of an attribute, affix the dollar sign $ to the object name followed by the attribute name. For example, to list the income values, type:\n\ns$Income\n\n [1] 45765 59560 46559 50515 50027 40695 55046 43484 56701 60782 52393 56139\n[13] 55045 70906 65226 79368 59580 56851 37378 41665 45747 44543 37110 39792\n[25] 38239 42407\n\n\nYou can plot the attribute values as follows:\n\nhist(s$Income, main=NULL)\n\n\n\n\nor,\n\nboxplot(s$Income, horizontal = TRUE)\n\n\n\n\nTo generate a map by symbolizing the polygons using the Income attribute we will define the classification breaks (breaks = quantile with n = 8 breaks) and the symbol colors (palette=\"Greens\"). For the latter, the tmap package makes use of Cynthia Brewer’s color schemes (you are already familiar with her website).\n\ncolor &lt;- brewer.pal(8, \"Greens\")\nplot(s[\"Income\"], key.pos = 4, nbreaks = 8, breaks = \"quantile\", pal = color)\n\n\n\n\nYou can change the classification schemes by setting the breaks parameter to styles such as \"equal\", \"jenks\" (ArcGIS’ default), \"sd\", \"pretty\" to name a few.\nYou can change the Brewer color palette to any one of the following sequential color schemes:\n\ndisplay.brewer.all(type = \"seq\")"
  },
  {
    "objectID": "moranI/Mapping_and_Morans.html#morans-i-analysis",
    "href": "moranI/Mapping_and_Morans.html#morans-i-analysis",
    "title": "Moran’s I analysis in R",
    "section": "Moran’s I analysis",
    "text": "Moran’s I analysis\n\nStep 1: Define neighboring polygons\nThe first step in a Moran’s I analysis requires that we define “neighboring” polygons. This could refer to contiguous polygons, polygons within a certain distance, or it could be non-spatial in nature and defined by social, political or cultural “neighbors”.\nHere, we’ll adopt a contiguous neighbor definition. We’ll accept any contiguous polygons that share at least one vertex; this is the “queen” case (if one chooses to adopt the chess analogy) and it’s parameterized as queen = TRUE in the call to poly2nb. If we required that just edges be shared between polygons then we would set queen = FALSE (the rook case).\n\nnb &lt;- poly2nb(s, queen=TRUE)\n\nFor each polygon in our shape object, nb lists all neighboring polygons. For example, to see the neighbors (by ID number) for the first polygon in the shape object, type:\n\nnb[1]\n\n[[1]]\n[1]  2  3  6  7 20\n\n\nHere’s the list of county names and associated IDs:\n\n\n\n\n\n\nCounty\nID\n\n\n\n\nAndroscoggin\n1\n\n\nCumberland\n2\n\n\nKennebec\n3\n\n\nKnox\n4\n\n\nLincoln\n5\n\n\nOxford\n6\n\n\nSagadahoc\n7\n\n\nWaldo\n8\n\n\nYork\n9\n\n\nBelknap\n10\n\n\nCarroll\n11\n\n\nCheshire\n12\n\n\nGrafton\n13\n\n\nHillsborough\n14\n\n\nMerrimack\n15\n\n\nRockingham\n16\n\n\nStrafford\n17\n\n\nSullivan\n18\n\n\nAroostook\n19\n\n\nFranklin\n20\n\n\nHancock\n21\n\n\nPenobscot\n22\n\n\nPiscataquis\n23\n\n\nSomerset\n24\n\n\nWashington\n25\n\n\nCoos\n26\n\n\n\n\n\n\n\n\n\nStep 2: Assign weights to the neighbors\nNext, we need to assign weights to each neighboring polygon. In this example, each neighboring polygon will be assigned equal weight when computing the neighboring mean income values.\n\nlw &lt;- nb2listw(nb, style=\"W\", zero.policy=TRUE)\n\nTo see the weight of the first polygon’s neighbors type:\n\nlw$weights[1]\n\n[[1]]\n[1] 0.2 0.2 0.2 0.2 0.2\n\n\nThese are the weights each neighboring income value will be multiplied by before being summed. If a polygon has 5 neighbors, each neighbor will have a weight of 1/5 or 0.2. This weight will then be used to compute the mean neighbor values as in 0.2(neighbor1) + 0.2(neighbor2) + 0.2(neighbor3) + 0.2(neighbor4) + 0.2(neighbor5). This is equivalent to summing all five income values then dividing by 5.\n\n\nStep 3: Compute the (weighted) neighbor mean income values (optional step)\nNOTE: This step does not need to be performed when running the moran or moran.test functions outlined in Steps 4 and 5. This step is only needed if you wish to generate a scatter plot between the income values and their lagged counterpart.\nNext, we’ll have R compute the average neighbor income value for each polygon. These values are often referred to as spatially lagged values.\n\ninc.lag &lt;- lag.listw(lw, s$Income)\ninc.lag\n\n [1] 48705.00 49551.75 45963.17 46755.50 48901.00 49748.50 50477.75 46197.17\n [9] 53057.00 58061.00 52535.00 63878.50 55531.80 64396.00 63755.33 65237.33\n[17] 62894.00 61829.00 39921.00 43202.75 42088.67 40291.67 40571.00 41789.83\n[25] 42556.00 49377.67\n\n\nYou can plot the relationship between income and its spatially lagged counterpart as follows (note that the blue line added to the plot is derived from a regression model).\n\nplot(inc.lag ~ s$Income, pch=16, asp=1)\nabline(lm(inc.lag ~ s$Income), col=\"blue\")\n\n\n\n\n\n\nStep 4: Computing the Moran’s I statistic\nThe Moran’s I statistic can be computed using the moran function.\n\nI &lt;- moran(s$Income, lw, length(nb), Szero(lw))[1]\nI\n\n$I\n[1] 0.6827955\n\n\nRecall that the Moran’s I value is the slope of the line that best fits the relationship between neighboring income values and each polygon’s income in the dataset.\n\n\nStep 5: Performing a hypothesis test\nThe hypothesis we are testing states that “the income values are randomly distributed across counties following a completely random process”. There are two methods to testing this hypothesis: an analytical method and a Monte Carlo method. We’ll explore both approaches in the following example.\n\nAnalytical method\nTo run the Moran’s I analysis using the analytical method, use the moran.test function.\n\nmoran.test(s$Income,lw, alternative=\"greater\")\n\n\n    Moran I test under randomisation\n\ndata:  s$Income  \nweights: lw    \n\nMoran I statistic standard deviate = 5.8525, p-value = 2.421e-09\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n       0.68279551       -0.04000000        0.01525284 \n\n\nThe Moran’s I statistic is 0.683 (same value that was computed using the moran function as expected). The p-value is very small. Usually, when the p-value is very small it’s common practice to report it as &lt; 0.001.\nNote that ArcGIS adopts this analytical approach to hypothesis testing however, it implements a two-sided test as opposed to the one-sided test adopted in the above example (i.e. alternative = \"greater\"). A two-sided p-value is nothing more than twice the one-sided p-value. Unfortunately, ArcGIS does not seem to make this important distinction in any of its documentation. This distinction can have important ramifications as shown in the next example (Florida crime data). The Maine income data is so strongly clustered that both a one-sided and two-sided test produce the same outcome (a p-value close to 0).\n\n\nMonte Carlo method\nThe analytical approach to the Moran’s I analysis benefits from being fast. But it may be sensitive to irregularly distributed polygons. A safer approach to hypothesis testing is to run an MC simulation using the moran.mc function. The number of simulations is defined by the nsim = parameter. Here, we’ll permute income values 999 times.\nThe moran.mc function takes another parameter called alternative =. This parameter has three possible values: \"greater\" (the default), \"less\", and \"two.sided\". The choice will be dictated by the side of the distribution we want to compute the p-value for. If our observed Moran’s I is to the right of the expected distribution, we will want to adopt the \"greater\" option which will focus on the upper tail of the distribution. If our observed value is to the left of the distribution, we will want to choose the \"less\" option to focus on the lower tail of the distribution. You can usually tell from the computed Moran’s I value which tail you will want to emphasize by its sign. A general rule of thumb is to place emphasis on the lower tail if Moran’s I value is negative, and to place emphasis on the upper tail if Moran’s I value is positive. In our example, out Moran’s I value of 0.68 is positive so we’ll choose \"greater\" for the parameter.\n\nMC&lt;- moran.mc(s$Income, lw, nsim = 999, alternative = \"greater\")\n\n# View results (including p-value)\nMC\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  s$Income \nweights: lw  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.6828, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\nThe MC simulation generates a very small p-value, 0.001. This is not surprising given that the income values are strongly clustered. We can see the results graphically by passing the Moran’s I model to the plot function:\n\n# Plot the Null distribution (note that this is a density plot instead of a histogram)\nplot(MC)\n\n\n\n\nThe curve shows the distribution of Moran I values we could expect had the incomes been randomly distributed across the counties. Note that our observed statistic, 0.683, falls way to the right of the distribution suggesting that the income values are clustered (a positive Moran’s I value suggests clustering whereas a negative Moran’s I value suggests dispersion).\nCan you tell the difference between our observed income distribution and those generated from a completely random process in the following figure?\n\n\n\n\n\nThe map on the left is our observed distribution. The three maps on the right are realizations of a completely random process."
  },
  {
    "objectID": "moranI/Mapping_and_Morans.html#another-example-florida-1980-homicide-rate-example",
    "href": "moranI/Mapping_and_Morans.html#another-example-florida-1980-homicide-rate-example",
    "title": "Moran’s I analysis in R",
    "section": "Another example: Florida 1980 Homicide rate example",
    "text": "Another example: Florida 1980 Homicide rate example\nIn this example, we explore the spatial distribution of 1980 homicide rates HR80 by county for the state of Florida using the Monte Carlo approach. The data are found in the NAT/ folder used in the in-class exercise.\n\n\n\n\n\nThe following code chunk highlights the entire workflow (don’t forget to set your R session folder to that which houses the FL.shp file).\n\n# Load the shapefile\ns &lt;- st_read(\"FL.shp\")\n\n# Define the neighbors (use queen case)\nnb &lt;- poly2nb(s, queen=TRUE)\n\n# Compute the neighboring average homicide rates\nlw &lt;- nb2listw(nb, style=\"W\", zero.policy=TRUE)\n\n# Run the MC simulation version of the Moran's I test\nM1 &lt;- moran.mc(s$HR80, lw, nsim=9999, alternative = \"greater\")\n\n# Plot the results\nplot(M1)\n\n# Display the resulting statistics\nM1\n\n\n\n\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  s$HR80 \nweights: lw  \nnumber of simulations + 1: 10000 \n\nstatistic = 0.13628, observed rank = 9575, p-value = 0.0425\nalternative hypothesis: greater\n\n\nThe MC simulation generated a p-value of ~0.04 suggesting that there would be a ~4% chance of being wrong in rejecting the null hypothesis or that there is a ~4% chance that our observed pattern is consistent with a random process (note that your simulated p-value may differ from the one shown here–the number of simulations may need to be increased to reach a more stable convergence). Recall that this is a one-sided test. ArcGIS’s analytical solution adopts a two-sided test. To compare its p-value to ours, we need to divide its p-value by 2 (i.e. 0.0588 / 2) which gives us a one-sided p-value of 0.0294–about 25% smaller than our simulated p-value.\n\n\n\n\n\nThe wording adopted by ArcGIS under the infographic (highlighted in yellow in the above figure) is unfortunate. It seems to suggest a one-sided test by explicitly describing the nature of the pattern (i.e. clustered). A more appropriate statement would have been “There is less than a 6% likelihood that the observed pattern could be the result of random chance” (note the omission of the word clustered)."
  },
  {
    "objectID": "moranI_distance/MoranI_distance_band.html",
    "href": "moranI_distance/MoranI_distance_band.html",
    "title": "Moran’s I in R: Distance bands",
    "section": "",
    "text": "Data for this tutorial can be downloaded from here. Don’t forget to unzip the files to a dedicated folder on your computer.\nWe will make use of the following packages:\nlibrary(sf)\nlibrary(spdep)\nlibrary(RColorBrewer)"
  },
  {
    "objectID": "moranI_distance/MoranI_distance_band.html#loading-and-visualizing-the-data",
    "href": "moranI_distance/MoranI_distance_band.html#loading-and-visualizing-the-data",
    "title": "Moran’s I in R: Distance bands",
    "section": "Loading and visualizing the data",
    "text": "Loading and visualizing the data\nLoad the Massachusetts income data (values are aggregated at the census tract level).\n\n# Load the shapefile\ns &lt;- st_read(\"MA_income.shp\")\n\nPlot the data.\n\ncolor &lt;- brewer.pal(8, \"Greens\")\nplot(s[\"Income\"], nbreaks = 8, breaks = \"quantile\", pal = color, border = NA)"
  },
  {
    "objectID": "moranI_distance/MoranI_distance_band.html#morans-i-as-a-function-of-a-distance-band",
    "href": "moranI_distance/MoranI_distance_band.html#morans-i-as-a-function-of-a-distance-band",
    "title": "Moran’s I in R: Distance bands",
    "section": "Moran’s I as a function of a distance band",
    "text": "Moran’s I as a function of a distance band\nWe will explore spatial autocorrelation as a function of distance bands. Instead of defining neighbors as contiguous polygons, we will define neighbors based on distances to polygon centers. A neighboring polygon will be defined as falling within a certain distance of a polygon of interest if its centroid (defined as a point) falls within the distance of the polygon of interest’s centroid. This will therefore require that we compute the centroid of each census tract polygon. The coordinate pairs can be extracted using the coordinate function.\n\ns.center &lt;- st_point_on_surface(s)\ns.coord &lt;- st_coordinates(s.center)\n\nThe object s.coord stores all pairs of coordinate values from the census tract.\nNext, we will define the search radius to include all neighboring polygon centers that are within 5 km (or 5,000 map units).\n\ns.dist  &lt;-  dnearneigh(s.coord, 0, 5000)  \n\nThe dnearneigh function takes three parameters: the coordinate values s.coord, the radius for the inner radius of the annulus band (first number in the function), and the radius for the outer annulus band (second number in the function). In our example, the inner annulus radius is 0 and the outer annulus radius is 5000 which implies that all polygon centers up to 5 km are considered neighbors (i.e. the current example creates a full circle search radius).\nAn example of polygons (in white) falling within 5 km of one MA census tract (in dark green) is shown in the following figure:\n\nNote that large polygons will have fewer neighbors given the increased distance between their centroids and neighboring centroids as seen in the following example:\n\nAs another example, if we chose to restrict the neighbors to all polygon centers between 40 km and 45 km, then we would define a search annulus (instead of a circle) as:\n\ns.dist  &lt;-  dnearneigh(s.coord, 40000, 45000) \n\nExamples of neighbors using this definition are shown next:\n\n\nContinuing with our 0 to 5 km distance band neighbor definition, we will create the list of neighboring polygons for each polygon in the dataset.\n\nlw &lt;- nb2listw(s.dist, style=\"W\", zero.policy = TRUE) \n\nNext, we’ll run the MC simulation.\n\nMI  &lt;-  moran.mc(s$Income, lw, nsim=599, zero.policy = TRUE) \n\nPlot the results.\n\nplot(MI, main = NULL) \n\n\n\n\nDisplay p-value and other summary statistics.\n\nMI\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  s$Income \nweights: lw  \nnumber of simulations + 1: 600 \n\nstatistic = 0.35028, observed rank = 600, p-value = 0.001667\nalternative hypothesis: greater\n\n\nIn this example, it’s clear that there is a strong spatial autocorrelation when defining neighbors as all census tracts within 5 km from the census tract of interest. None of the simulated Moran’s I values under the null hypothesis were more extreme than our observed Moran I value.\nNow, let’s run a Moran’s I analysis using 40 to 45 km distance band:\n\ns.dist  &lt;-  dnearneigh(s.coord, 40000, 45000)\nlw &lt;- nb2listw(s.dist, style=\"W\", zero.policy = TRUE) \nMI  &lt;-  moran.mc(s$Income, lw, nsim=599, zero.policy = TRUE) \nplot(MI, main = NULL) \n\n\n\nMI       \n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  s$Income \nweights: lw  \nnumber of simulations + 1: 600 \n\nstatistic = -0.012984, observed rank = 13, p-value = 0.9783\nalternative hypothesis: greater\n\n\nThe output differs from that shown earlier. Here, the Moran’s I analysis suggests a slightly more dispersed pattern than expected when comparing income values 40 to 45 km apart from one another."
  },
  {
    "objectID": "moranI_distance/MoranI_distance_band.html#morans-i-as-a-function-of-multiple-distance-bands",
    "href": "moranI_distance/MoranI_distance_band.html#morans-i-as-a-function-of-multiple-distance-bands",
    "title": "Moran’s I in R: Distance bands",
    "section": "Moran’s I as a function of multiple distance bands",
    "text": "Moran’s I as a function of multiple distance bands\n\nNote that you will not be expected to run this script in your final exam. But you should be able to interpret the output if asked.\n\nWe can extend this analysis by calculating a Moran’s I index (and p.value) at different distance bands. This gives us the change in spatial autocorrelation as a function of distance.\nWe first create a “for” loop that iterates across range of distance band values.\n\n# Define the distance bands (note that each band is defined\n# by an annulus)\nstart &lt;- 0 # Starting distance in meters (the From)\nend &lt;- 125000 # Ending distance in meters (the To)\nincr &lt;- 5000 # Distance increment (which also defines the annulus width)\nincr.v &lt;- seq(start, end, incr)\n\n\n# Define empty vector elements to store the I and p-values\nmorI.mc &lt;- vector()\nsign.mc &lt;- vector()\n\n# Loop through each distance band\nfor (i in (2:length(incr.v))) {\n  s.dist &lt;- dnearneigh(s.coord, incr.v[i - 1], incr.v[i])\n  s.lw &lt;- nb2listw(s.dist, style = \"W\", zero.policy=T)\n  s.mor &lt;- moran.mc(s$Income, s.lw, nsim=599, zero.policy = TRUE)\n  sign.mc[i] &lt;- s.mor$p.value\n  morI.mc[i] &lt;- s.mor$statistic\n}\n\n# Modify p-value to reflect extremes at each end\nsign.mc &lt;- ifelse(sign.mc &gt; 0.5, 1 - sign.mc, sign.mc)\n\nNext, we’ll plot the Moran’s I statistic as a function of distance band. We’ll also symbolize each point by a binary color scheme whereby “red” will be assigned to Moran’s I dots that meet a statistical threshold (let’s say 0.01 in this example) and a “grey” dot otherwise.\n\n# First, generate an empty plot\nplot(morI.mc ~ eval(incr.v - incr * 0.5), type = \"n\", ann = FALSE, axes = FALSE)\n\n# Set the background plot to grey then add white grids\nu &lt;- par(\"usr\") # Get plot are coordinates\nrect(u[1], u[3], u[2], u[4], col = \"#EEEEEE\", border = NA)\naxis(1, lab = ((incr.v) / 1000), at = (incr.v), tck = 1, col = \"#FFFFFF\", lty = 1)\naxis(2, tck = 1, col = \"#FFFFFF\", lty = 1, labels = FALSE)\n\n# Add the theoretical \"no autocorelation\" line\nabline(h = -1 / (length(s$Income)), col = \"grey20\")\n\n# Add the plot to the canvas\npar(new = TRUE)\nplot(morI.mc ~ eval(incr.v - incr * 0.5),\n     type = \"b\", xaxt = \"n\", las = 1,\n     xlab = \"Distance (km)\", ylab = \"Moran's I\")\npoints(morI.mc ~ eval(incr.v - incr * 0.5), \n       col = ifelse(sign.mc &lt; 0.01, \"red\", \"grey\"), \n       pch = 16, cex = 2.0)\n\n# Add numeric values to points\ntext(eval(incr.v - incr * 0.5), morI.mc, round(sign.mc,3), pos = 3, cex = 0.5)"
  },
  {
    "objectID": "PPM/PPM_analysis.html",
    "href": "PPM/PPM_analysis.html",
    "title": "PPM: Exploring 1st order effects",
    "section": "",
    "text": "Data for this tutorial can be downloaded from here. Don’t forget to unzip the files to a dedicated folder on your computer."
  },
  {
    "objectID": "PPM/PPM_analysis.html#loading-the-data-into-r",
    "href": "PPM/PPM_analysis.html#loading-the-data-into-r",
    "title": "PPM: Exploring 1st order effects",
    "section": "Loading the data into R",
    "text": "Loading the data into R\nFirst, we’ll load the point pattern dataset, then we’ll load two different raster datasets that will represent two separate 1st order effects we suspect may help explain the distribution of stores within the study extent.\n\n# Load packages\nlibrary(spatstat)\nlibrary(sf)\nlibrary(terra)\n\n# Read state polygon data\ns    &lt;- st_read(\"MA.shp\")\nw    &lt;- as.owin(s)\nw.km &lt;- rescale.owin(w, 1000)\n\n# Read Walmart point data\ns    &lt;- st_read(\"Walmarts.shp\")\np    &lt;- as.ppp(s)\np.km &lt;- rescale.ppp(p, 1000)\nmarks(p.km)  &lt;- NULL\nWindow(p.km) &lt;- w.km\n\n# Read population density raster\nimg     &lt;- rast(\"log_pop_sqmile.tif\")\ndf      &lt;- as.data.frame(img, xy = TRUE)\npop     &lt;- as.im(df)\npop.km  &lt;- rescale.im(pop, 1000)\n\n# Read median income raster\nimg      &lt;- rast(\"median_income.tif\")\ndf       &lt;- as.data.frame(img, xy = TRUE)\ninc      &lt;- as.im(df)\ninc.km   &lt;- rescale.im(inc, 1000)\n\nNote that later in the script, we will make use of the effectfun() that requires that the raster layer be stored as a double data type. You can tell if your raster is stored as a double by typing typeof(inc.km[]), for example. If it’s not stored as a double, you can force it to a double using the as.double() function (e.g. inc[] &lt;- as.double(inc[])).\nLet’s plot each raster with the Walmart point overlay. We’ll use R’s base plotting environment.\n\nplot(pop.km, ribside=\"bottom\", main=\"Population (log)\")\nplot(p.km, pch = 20, col=rgb(1,1,1,0.5), add=TRUE)\n\nplot(inc.km, ribside=\"bottom\", main=\"Income\")\nplot(p.km, pch = 20, col=rgb(1,1,1,0.6), add=TRUE)"
  },
  {
    "objectID": "PPM/PPM_analysis.html#modeling-point-density-as-a-function-of-two-competing-covariates-population-density-and-income.",
    "href": "PPM/PPM_analysis.html#modeling-point-density-as-a-function-of-two-competing-covariates-population-density-and-income.",
    "title": "PPM: Exploring 1st order effects",
    "section": "Modeling point density as a function of two competing covariates: population density and income.",
    "text": "Modeling point density as a function of two competing covariates: population density and income.\nWe will first develop two different models that we think might define the Walmart’s point intensity. Recall that intensity is a property of the underlying “process” (one being defined by the distribution of population density and the other being defined by the distribution of income)–this differs from density which is associated with the observed pattern. These models are represented in our R script by two raster layers: a population raster and an income raster. These models will be alternate models that we’ll denote as Mpop and Minc in the script. The models’ structure will follow the form of a logistics model, but note that the models can take on many different forms and different levels of complexity.\nWe’ll also create the null model, Mo, which will assume a spatially uniform (homogeneous) covariate. In other words, Mo will define the model where the intensity of the process is assumed to be the same across the entire study extent.\n\nMpop &lt;- ppm(p.km ~ pop.km) # Population model\n\nWarning: Values of the covariate 'pop.km' were NA or undefined at 3.4% (17 out\nof 504) of the quadrature points. Occurred while executing: ppm.ppp(Q = p.km,\ntrend = ~pop.km, data = NULL, interaction = NULL)\n\nMinc &lt;- ppm(p.km ~ inc.km) # Income model\n\nWarning: Values of the covariate 'inc.km' were NA or undefined at 3.2% (16 out\nof 504) of the quadrature points. Occurred while executing: ppm.ppp(Q = p.km,\ntrend = ~inc.km, data = NULL, interaction = NULL)\n\nMo   &lt;- ppm(p.km ~ 1)      # Null model\n\nLet’s explore the model parameters. First, we’ll look at Mpop.\n\nMpop\n\nNonstationary Poisson process\nFitted to point pattern dataset 'p.km'\n\nLog intensity:  ~pop.km\n\nFitted trend coefficients:\n(Intercept)      pop.km \n -10.111647    0.625803 \n\n              Estimate      S.E.     CI95.lo    CI95.hi Ztest       Zval\n(Intercept) -10.111647 0.7933272 -11.6665394 -8.5567540   *** -12.745872\npop.km        0.625803 0.1127980   0.4047229  0.8468831   ***   5.547996\nProblem:\n Values of the covariate 'pop.km' were NA or undefined at 3.4% (17 out of 504) \nof the quadrature points\n\n\nThe values of interest are the intercept (whose value is around -10.1) and the coefficient pop.km (whose value is around 0.63). Using these values, we can construct the mathematical relationship (noting that we are using the logged population raster and not the original population raster values):\n\\[\nWalmart\\ intensity(i)= e^{−10.1 + 0.63\\ log(population\\ density_i)}\n\\] The above equation can be interpreted as follows: if the population density is 0, then the Walmart intensity is \\(e^{−10.1}\\) which is very close to 0. So for every unit increase of the logged population density (i.e. log of one person per square mile), there is an increase in Walmart intensity by a factor of \\(e^{0.63}\\).\nLikewise, we can extract the parameters from the Minc model and construct its equation.\n\nMinc\n\nNonstationary Poisson process\nFitted to point pattern dataset 'p.km'\n\nLog intensity:  ~inc.km\n\nFitted trend coefficients:\n  (Intercept)        inc.km \n-5.576942e+00 -1.654792e-05 \n\n                 Estimate         S.E.       CI95.lo       CI95.hi Ztest\n(Intercept) -5.576942e+00 5.258426e-01 -6.607574e+00 -4.546309e+00   ***\ninc.km      -1.654792e-05 1.490546e-05 -4.576209e-05  1.266624e-05      \n                  Zval\n(Intercept) -10.605725\ninc.km       -1.110192\nProblem:\n Values of the covariate 'inc.km' were NA or undefined at 3.2% (16 out of 504) \nof the quadrature points\n\n\n\\[\nWalmart\\ intensity(i) = e^{−5.58\\ −1.66e^{−5}\\ Income(i)}\n\\] Note the negative (decreasing) relationship between income distribution and Walmart density.\nNext, we’ll extract the null model results:\n\nMo\n\nStationary Poisson process\nFitted to point pattern dataset 'p.km'\nIntensity: 0.002086305\n             Estimate      S.E.   CI95.lo   CI95.hi Ztest      Zval\nlog(lambda) -6.172361 0.1507557 -6.467836 -5.876885   *** -40.94281\n\n\nThis gives us the following equation for the homogeneous process:\n\\[\nWalmart\\ intensity(i) = e^{−6.17} = 0.00209\n\\]\nwhich is nothing more than the number of stores per unit area (44 stores / 21,000km2 = 0.00209)."
  },
  {
    "objectID": "PPM/PPM_analysis.html#plotting-the-competing-models",
    "href": "PPM/PPM_analysis.html#plotting-the-competing-models",
    "title": "PPM: Exploring 1st order effects",
    "section": "Plotting the competing models",
    "text": "Plotting the competing models\n\nOP &lt;- par(mfrow = c(1,2), mar = c(4,4,2,1))  # This creates a two-pane plotting window\n\nplot(effectfun(Mpop, \"pop.km\", se.fit = TRUE), main = \"Population\",\n     ylab = \"Walmarts per km2\", xlab = \"Population density\", legend = FALSE)\n\nplot(effectfun(Minc, \"inc.km\", se.fit = TRUE), main = \"Income\",\n     ylab = \"Walmarts per km2\", xlab = \"Income\", legend = FALSE)\n\npar(OP) # This reverts our plot window back to a one-pane window\n\n\n\n\nNote the difference in relationships between the two models. In the first plot, we note an increasing relationship between Walmart intensity and population density; this is to be expected since you would not expect to see Walmart stores in underpopulated areas. In the second plot, we note an inverse relationship between Walmart intensity and income—i.e. as an area’s income increases, the Walmart intensity decreases.\nThe grey envelopes encompass the 95% confidence interval; i.e. the true estimate (black line) can fall anywhere within this envelope. Note how the envelope broadens near the upper end of the population density values–this suggests wide uncertainty in the estimated model.\nTo assess how well the above models explain the relationship between covariate and Walmart intensity, we will turn to hypothesis testing."
  },
  {
    "objectID": "PPM/PPM_analysis.html#testing-for-covariate-effect",
    "href": "PPM/PPM_analysis.html#testing-for-covariate-effect",
    "title": "PPM: Exploring 1st order effects",
    "section": "Testing for covariate effect",
    "text": "Testing for covariate effect\nNow, let’s compare the non-homogeneous covariates to the null model using a technique called the likelihood ratio test. Remember that the null model assumes that the intensity is homogeneous across the entire study area; what we want to know is “does the model with the covariate do a significantly better job in predicting Walmart densities than the null model?”\n\nanova(Mo, Mpop, test = \"LRT\") # Compare null to population model\n\nWarning: Values of the covariate 'pop.km' were NA or undefined at 3.4% (17 out\nof 504) of the quadrature points. Occurred while executing: ppm.ppp(Q = p.km,\ntrend = ~pop.km, data = NULL, interaction = NULL,\n\n\nWarning: Models were re-fitted after discarding quadrature points that were\nillegal under some of the models\n\n\nAnalysis of Deviance Table\n\nModel 1: ~1      Poisson\nModel 2: ~pop.km     Poisson\n  Npar Df Deviance  Pr(&gt;Chi)    \n1   18                          \n2   19  1   31.725 1.776e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nanova(Mo, Minc, test = \"LRT\") # Compare null to income model\n\nWarning: Values of the covariate 'inc.km' were NA or undefined at 3.2% (16 out\nof 504) of the quadrature points. Occurred while executing: ppm.ppp(Q = p.km,\ntrend = ~inc.km, data = NULL, interaction = NULL,\n\n\nWarning: Models were re-fitted after discarding quadrature points that were\nillegal under some of the models\n\n\nAnalysis of Deviance Table\n\nModel 1: ~1      Poisson\nModel 2: ~inc.km     Poisson\n  Npar Df Deviance Pr(&gt;Chi)\n1   17                     \n2   18  1   1.3388   0.2473\n\n\nWhat we are seeking is a small p-value (parameter Pr(&gt;Chi) in the output). The smaller the value, the more confident we are in stating that the covariate does a better job in predicting Walmart intensity than the null model. For example, the p-value for the Mpop model (Pr(&gt;Chi) = 1.776e-08) suggests that population density does a better job in predicting Walmart density than the null model Mo.\nThe p-value for Minc, on the other hand, is higher with a value of Pr = 0.247 indicating that there is a 24.7% chance that we would be wrong in stating that income does a better job in predicting Walmart densities. To many, that probability is too high to reject the null.\nSo to summaries: of the two models we tested, it seems that population density does a better job at explaining the distribution of Walmarts (though not perfect) than the null model. Income distribution, on the other hand, does not improve on the null model."
  },
  {
    "objectID": "index.html#introduction-to-r",
    "href": "index.html#introduction-to-r",
    "title": "This repo houses standalone R scripts used in Colby’s ES214 course",
    "section": "",
    "text": "Loading and visualizing spatial data in R"
  },
  {
    "objectID": "Getting_started_with_R/Getting_started_with_R.html#setting-an-r-sessions-work-space",
    "href": "Getting_started_with_R/Getting_started_with_R.html#setting-an-r-sessions-work-space",
    "title": "Loading and visualizing data in R",
    "section": "Setting an R session’s work space",
    "text": "Setting an R session’s work space\nIf you plan to read or write files from/to a directory, you might find it beneficial to explicitly define the R session’s project folder. To set a session’s working directory, go to Session &gt;&gt; Set Working Directory &gt;&gt; Choose Directory. In this example, you will want to set the working directory to the folder that houses the sample dataset."
  }
]